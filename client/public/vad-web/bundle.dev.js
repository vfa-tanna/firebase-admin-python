/*
 * ATTENTION: The "eval" devtool has been used (maybe by default in mode: "development").
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
(function webpackUniversalModuleDefinition(root, factory) {
	if(typeof exports === 'object' && typeof module === 'object')
		module.exports = factory(require("onnxruntime-web"));
	else if(typeof define === 'function' && define.amd)
		define(["onnxruntime-web"], factory);
	else if(typeof exports === 'object')
		exports["vad"] = factory(require("onnxruntime-web"));
	else
		root["vad"] = factory(root["ort"]);
})(self, (__WEBPACK_EXTERNAL_MODULE_onnxruntime_web__) => {
return /******/ (() => { // webpackBootstrap
/******/ 	var __webpack_modules__ = ({

/***/ "../../node_modules/onnxruntime-web/dist/ort.wasm.min.js":
/*!***************************************************************!*\
  !*** ../../node_modules/onnxruntime-web/dist/ort.wasm.min.js ***!
  \***************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

"use strict";
eval("/*!\n * ONNX Runtime Web v1.22.0\n * Copyright (c) Microsoft Corporation. All rights reserved.\n * Licensed under the MIT License.\n */\nvar ort=(()=>{var je=Object.defineProperty;var An=Object.getOwnPropertyDescriptor;var On=Object.getOwnPropertyNames;var In=Object.prototype.hasOwnProperty;var Ve=(e=> true?__webpack_require__(\"../../node_modules/onnxruntime-web/dist sync recursive\"):0)(function(e){if(true)return __webpack_require__(\"../../node_modules/onnxruntime-web/dist sync recursive\").apply(this,arguments);throw Error('Dynamic require of \"'+e+'\" is not supported')});var E=(e,t)=>()=>(e&&(t=e(e=0)),t);var be=(e,t)=>{for(var n in t)je(e,n,{get:t[n],enumerable:!0})},Pn=(e,t,n,o)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let r of On(t))!In.call(e,r)&&r!==n&&je(e,r,{get:()=>t[r],enumerable:!(o=An(t,r))||o.enumerable});return e};var Ye=e=>Pn(je({},\"__esModule\",{value:!0}),e);var ge,J,re,Ln,mt,qe=E(()=>{\"use strict\";ge=new Map,J=[],re=(e,t,n)=>{if(t&&typeof t.init==\"function\"&&typeof t.createInferenceSessionHandler==\"function\"){let o=ge.get(e);if(o===void 0)ge.set(e,{backend:t,priority:n});else{if(o.priority>n)return;if(o.priority===n&&o.backend!==t)throw new Error(`cannot register backend \"${e}\" using priority ${n}`)}if(n>=0){let r=J.indexOf(e);r!==-1&&J.splice(r,1);for(let i=0;i<J.length;i++)if(ge.get(J[i]).priority<=n){J.splice(i,0,e);return}J.push(e)}return}throw new TypeError(\"not a valid backend\")},Ln=async e=>{let t=ge.get(e);if(!t)return\"backend not found.\";if(t.initialized)return t.backend;if(t.aborted)return t.error;{let n=!!t.initPromise;try{return n||(t.initPromise=t.backend.init(e)),await t.initPromise,t.initialized=!0,t.backend}catch(o){return n||(t.error=`${o}`,t.aborted=!0),t.error}finally{delete t.initPromise}}},mt=async e=>{let t=e.executionProviders||[],n=t.map(u=>typeof u==\"string\"?u:u.name),o=n.length===0?J:n,r,i=[],a=new Set;for(let u of o){let f=await Ln(u);typeof f==\"string\"?i.push({name:u,err:f}):(r||(r=f),r===f&&a.add(u))}if(!r)throw new Error(`no available backend found. ERR: ${i.map(u=>`[${u.name}] ${u.err}`).join(\", \")}`);for(let{name:u,err:f}of i)n.includes(u)&&console.warn(`removing requested execution provider \"${u}\" from session options because it is not available: ${f}`);let s=t.filter(u=>a.has(typeof u==\"string\"?u:u.name));return[r,new Proxy(e,{get:(u,f)=>f===\"executionProviders\"?s:Reflect.get(u,f)})]}});var wt=E(()=>{\"use strict\";qe()});var ht,yt=E(()=>{\"use strict\";ht=\"1.22.0\"});var bt,C,Je=E(()=>{\"use strict\";yt();bt=\"warning\",C={wasm:{},webgl:{},webgpu:{},versions:{common:ht},set logLevel(e){if(e!==void 0){if(typeof e!=\"string\"||[\"verbose\",\"info\",\"warning\",\"error\",\"fatal\"].indexOf(e)===-1)throw new Error(`Unsupported logging level: ${e}`);bt=e}},get logLevel(){return bt}};Object.defineProperty(C,\"logLevel\",{enumerable:!0})});var O,gt=E(()=>{\"use strict\";Je();O=C});var Et,Tt,St=E(()=>{\"use strict\";Et=(e,t)=>{let n=typeof document<\"u\"?document.createElement(\"canvas\"):new OffscreenCanvas(1,1);n.width=e.dims[3],n.height=e.dims[2];let o=n.getContext(\"2d\");if(o!=null){let r,i;t?.tensorLayout!==void 0&&t.tensorLayout===\"NHWC\"?(r=e.dims[2],i=e.dims[3]):(r=e.dims[3],i=e.dims[2]);let a=t?.format!==void 0?t.format:\"RGB\",s=t?.norm,u,f;s===void 0||s.mean===void 0?u=[255,255,255,255]:typeof s.mean==\"number\"?u=[s.mean,s.mean,s.mean,s.mean]:(u=[s.mean[0],s.mean[1],s.mean[2],0],s.mean[3]!==void 0&&(u[3]=s.mean[3])),s===void 0||s.bias===void 0?f=[0,0,0,0]:typeof s.bias==\"number\"?f=[s.bias,s.bias,s.bias,s.bias]:(f=[s.bias[0],s.bias[1],s.bias[2],0],s.bias[3]!==void 0&&(f[3]=s.bias[3]));let c=i*r,l=0,d=c,p=c*2,w=-1;a===\"RGBA\"?(l=0,d=c,p=c*2,w=c*3):a===\"RGB\"?(l=0,d=c,p=c*2):a===\"RBG\"&&(l=0,p=c,d=c*2);for(let b=0;b<i;b++)for(let I=0;I<r;I++){let m=(e.data[l++]-f[0])*u[0],h=(e.data[d++]-f[1])*u[1],P=(e.data[p++]-f[2])*u[2],g=w===-1?255:(e.data[w++]-f[3])*u[3];o.fillStyle=\"rgba(\"+m+\",\"+h+\",\"+P+\",\"+g+\")\",o.fillRect(I,b,1,1)}if(\"toDataURL\"in n)return n.toDataURL();throw new Error(\"toDataURL is not supported\")}else throw new Error(\"Can not access image data\")},Tt=(e,t)=>{let n=typeof document<\"u\"?document.createElement(\"canvas\").getContext(\"2d\"):new OffscreenCanvas(1,1).getContext(\"2d\"),o;if(n!=null){let r,i,a;t?.tensorLayout!==void 0&&t.tensorLayout===\"NHWC\"?(r=e.dims[2],i=e.dims[1],a=e.dims[3]):(r=e.dims[3],i=e.dims[2],a=e.dims[1]);let s=t!==void 0&&t.format!==void 0?t.format:\"RGB\",u=t?.norm,f,c;u===void 0||u.mean===void 0?f=[255,255,255,255]:typeof u.mean==\"number\"?f=[u.mean,u.mean,u.mean,u.mean]:(f=[u.mean[0],u.mean[1],u.mean[2],255],u.mean[3]!==void 0&&(f[3]=u.mean[3])),u===void 0||u.bias===void 0?c=[0,0,0,0]:typeof u.bias==\"number\"?c=[u.bias,u.bias,u.bias,u.bias]:(c=[u.bias[0],u.bias[1],u.bias[2],0],u.bias[3]!==void 0&&(c[3]=u.bias[3]));let l=i*r;if(t!==void 0&&(t.format!==void 0&&a===4&&t.format!==\"RGBA\"||a===3&&t.format!==\"RGB\"&&t.format!==\"BGR\"))throw new Error(\"Tensor format doesn't match input tensor dims\");let d=4,p=0,w=1,b=2,I=3,m=0,h=l,P=l*2,g=-1;s===\"RGBA\"?(m=0,h=l,P=l*2,g=l*3):s===\"RGB\"?(m=0,h=l,P=l*2):s===\"RBG\"&&(m=0,P=l,h=l*2),o=n.createImageData(r,i);for(let S=0;S<i*r;p+=d,w+=d,b+=d,I+=d,S++)o.data[p]=(e.data[m++]-c[0])*f[0],o.data[w]=(e.data[h++]-c[1])*f[1],o.data[b]=(e.data[P++]-c[2])*f[2],o.data[I]=g===-1?255:(e.data[g++]-c[3])*f[3]}else throw new Error(\"Can not access image data\");return o}});var Ze,At,Ot,It,Pt,Lt,xt=E(()=>{\"use strict\";Ee();Ze=(e,t)=>{if(e===void 0)throw new Error(\"Image buffer must be defined\");if(t.height===void 0||t.width===void 0)throw new Error(\"Image height and width must be defined\");if(t.tensorLayout===\"NHWC\")throw new Error(\"NHWC Tensor layout is not supported yet\");let{height:n,width:o}=t,r=t.norm??{mean:255,bias:0},i,a;typeof r.mean==\"number\"?i=[r.mean,r.mean,r.mean,r.mean]:i=[r.mean[0],r.mean[1],r.mean[2],r.mean[3]??255],typeof r.bias==\"number\"?a=[r.bias,r.bias,r.bias,r.bias]:a=[r.bias[0],r.bias[1],r.bias[2],r.bias[3]??0];let s=t.format!==void 0?t.format:\"RGBA\",u=t.tensorFormat!==void 0&&t.tensorFormat!==void 0?t.tensorFormat:\"RGB\",f=n*o,c=u===\"RGBA\"?new Float32Array(f*4):new Float32Array(f*3),l=4,d=0,p=1,w=2,b=3,I=0,m=f,h=f*2,P=-1;s===\"RGB\"&&(l=3,d=0,p=1,w=2,b=-1),u===\"RGBA\"?P=f*3:u===\"RBG\"?(I=0,h=f,m=f*2):u===\"BGR\"&&(h=0,m=f,I=f*2);for(let S=0;S<f;S++,d+=l,w+=l,p+=l,b+=l)c[I++]=(e[d]+a[0])/i[0],c[m++]=(e[p]+a[1])/i[1],c[h++]=(e[w]+a[2])/i[2],P!==-1&&b!==-1&&(c[P++]=(e[b]+a[3])/i[3]);return u===\"RGBA\"?new U(\"float32\",c,[1,4,n,o]):new U(\"float32\",c,[1,3,n,o])},At=async(e,t)=>{let n=typeof HTMLImageElement<\"u\"&&e instanceof HTMLImageElement,o=typeof ImageData<\"u\"&&e instanceof ImageData,r=typeof ImageBitmap<\"u\"&&e instanceof ImageBitmap,i=typeof e==\"string\",a,s=t??{},u=()=>{if(typeof document<\"u\")return document.createElement(\"canvas\");if(typeof OffscreenCanvas<\"u\")return new OffscreenCanvas(1,1);throw new Error(\"Canvas is not supported\")},f=c=>typeof HTMLCanvasElement<\"u\"&&c instanceof HTMLCanvasElement||c instanceof OffscreenCanvas?c.getContext(\"2d\"):null;if(n){let c=u();c.width=e.width,c.height=e.height;let l=f(c);if(l!=null){let d=e.height,p=e.width;if(t!==void 0&&t.resizedHeight!==void 0&&t.resizedWidth!==void 0&&(d=t.resizedHeight,p=t.resizedWidth),t!==void 0){if(s=t,t.tensorFormat!==void 0)throw new Error(\"Image input config format must be RGBA for HTMLImageElement\");s.tensorFormat=\"RGBA\",s.height=d,s.width=p}else s.tensorFormat=\"RGBA\",s.height=d,s.width=p;l.drawImage(e,0,0),a=l.getImageData(0,0,p,d).data}else throw new Error(\"Can not access image data\")}else if(o){let c,l;if(t!==void 0&&t.resizedWidth!==void 0&&t.resizedHeight!==void 0?(c=t.resizedHeight,l=t.resizedWidth):(c=e.height,l=e.width),t!==void 0&&(s=t),s.format=\"RGBA\",s.height=c,s.width=l,t!==void 0){let d=u();d.width=l,d.height=c;let p=f(d);if(p!=null)p.putImageData(e,0,0),a=p.getImageData(0,0,l,c).data;else throw new Error(\"Can not access image data\")}else a=e.data}else if(r){if(t===void 0)throw new Error(\"Please provide image config with format for Imagebitmap\");let c=u();c.width=e.width,c.height=e.height;let l=f(c);if(l!=null){let d=e.height,p=e.width;return l.drawImage(e,0,0,p,d),a=l.getImageData(0,0,p,d).data,s.height=d,s.width=p,Ze(a,s)}else throw new Error(\"Can not access image data\")}else{if(i)return new Promise((c,l)=>{let d=u(),p=f(d);if(!e||!p)return l();let w=new Image;w.crossOrigin=\"Anonymous\",w.src=e,w.onload=()=>{d.width=w.width,d.height=w.height,p.drawImage(w,0,0,d.width,d.height);let b=p.getImageData(0,0,d.width,d.height);s.height=d.height,s.width=d.width,c(Ze(b.data,s))}});throw new Error(\"Input data provided is not supported - aborted tensor creation\")}if(a!==void 0)return Ze(a,s);throw new Error(\"Input data provided is not supported - aborted tensor creation\")},Ot=(e,t)=>{let{width:n,height:o,download:r,dispose:i}=t,a=[1,o,n,4];return new U({location:\"texture\",type:\"float32\",texture:e,dims:a,download:r,dispose:i})},It=(e,t)=>{let{dataType:n,dims:o,download:r,dispose:i}=t;return new U({location:\"gpu-buffer\",type:n??\"float32\",gpuBuffer:e,dims:o,download:r,dispose:i})},Pt=(e,t)=>{let{dataType:n,dims:o,download:r,dispose:i}=t;return new U({location:\"ml-tensor\",type:n??\"float32\",mlTensor:e,dims:o,download:r,dispose:i})},Lt=(e,t,n)=>new U({location:\"cpu-pinned\",type:e,data:t,dims:n??[t.length]})});var Z,de,vt,Bt,Ut=E(()=>{\"use strict\";Z=new Map([[\"float32\",Float32Array],[\"uint8\",Uint8Array],[\"int8\",Int8Array],[\"uint16\",Uint16Array],[\"int16\",Int16Array],[\"int32\",Int32Array],[\"bool\",Uint8Array],[\"float64\",Float64Array],[\"uint32\",Uint32Array],[\"int4\",Uint8Array],[\"uint4\",Uint8Array]]),de=new Map([[Float32Array,\"float32\"],[Uint8Array,\"uint8\"],[Int8Array,\"int8\"],[Uint16Array,\"uint16\"],[Int16Array,\"int16\"],[Int32Array,\"int32\"],[Float64Array,\"float64\"],[Uint32Array,\"uint32\"]]),vt=!1,Bt=()=>{if(!vt){vt=!0;let e=typeof BigInt64Array<\"u\"&&BigInt64Array.from,t=typeof BigUint64Array<\"u\"&&BigUint64Array.from,n=globalThis.Float16Array,o=typeof n<\"u\"&&n.from;e&&(Z.set(\"int64\",BigInt64Array),de.set(BigInt64Array,\"int64\")),t&&(Z.set(\"uint64\",BigUint64Array),de.set(BigUint64Array,\"uint64\")),o?(Z.set(\"float16\",n),de.set(n,\"float16\")):Z.set(\"float16\",Uint16Array)}}});var _t,Mt,Dt=E(()=>{\"use strict\";Ee();_t=e=>{let t=1;for(let n=0;n<e.length;n++){let o=e[n];if(typeof o!=\"number\"||!Number.isSafeInteger(o))throw new TypeError(`dims[${n}] must be an integer, got: ${o}`);if(o<0)throw new RangeError(`dims[${n}] must be a non-negative integer, got: ${o}`);t*=o}return t},Mt=(e,t)=>{switch(e.location){case\"cpu\":return new U(e.type,e.data,t);case\"cpu-pinned\":return new U({location:\"cpu-pinned\",data:e.data,type:e.type,dims:t});case\"texture\":return new U({location:\"texture\",texture:e.texture,type:e.type,dims:t});case\"gpu-buffer\":return new U({location:\"gpu-buffer\",gpuBuffer:e.gpuBuffer,type:e.type,dims:t});case\"ml-tensor\":return new U({location:\"ml-tensor\",mlTensor:e.mlTensor,type:e.type,dims:t});default:throw new Error(`tensorReshape: tensor location ${e.location} is not supported`)}}});var U,Ee=E(()=>{\"use strict\";St();xt();Ut();Dt();U=class{constructor(t,n,o){Bt();let r,i;if(typeof t==\"object\"&&\"location\"in t)switch(this.dataLocation=t.location,r=t.type,i=t.dims,t.location){case\"cpu-pinned\":{let s=Z.get(r);if(!s)throw new TypeError(`unsupported type \"${r}\" to create tensor from pinned buffer`);if(!(t.data instanceof s))throw new TypeError(`buffer should be of type ${s.name}`);this.cpuData=t.data;break}case\"texture\":{if(r!==\"float32\")throw new TypeError(`unsupported type \"${r}\" to create tensor from texture`);this.gpuTextureData=t.texture,this.downloader=t.download,this.disposer=t.dispose;break}case\"gpu-buffer\":{if(r!==\"float32\"&&r!==\"float16\"&&r!==\"int32\"&&r!==\"int64\"&&r!==\"uint32\"&&r!==\"uint8\"&&r!==\"bool\"&&r!==\"uint4\"&&r!==\"int4\")throw new TypeError(`unsupported type \"${r}\" to create tensor from gpu buffer`);this.gpuBufferData=t.gpuBuffer,this.downloader=t.download,this.disposer=t.dispose;break}case\"ml-tensor\":{if(r!==\"float32\"&&r!==\"float16\"&&r!==\"int32\"&&r!==\"int64\"&&r!==\"uint32\"&&r!==\"uint64\"&&r!==\"int8\"&&r!==\"uint8\"&&r!==\"bool\"&&r!==\"uint4\"&&r!==\"int4\")throw new TypeError(`unsupported type \"${r}\" to create tensor from MLTensor`);this.mlTensorData=t.mlTensor,this.downloader=t.download,this.disposer=t.dispose;break}default:throw new Error(`Tensor constructor: unsupported location '${this.dataLocation}'`)}else{let s,u;if(typeof t==\"string\")if(r=t,u=o,t===\"string\"){if(!Array.isArray(n))throw new TypeError(\"A string tensor's data must be a string array.\");s=n}else{let f=Z.get(t);if(f===void 0)throw new TypeError(`Unsupported tensor type: ${t}.`);if(Array.isArray(n)){if(t===\"float16\"&&f===Uint16Array||t===\"uint4\"||t===\"int4\")throw new TypeError(`Creating a ${t} tensor from number array is not supported. Please use ${f.name} as data.`);t===\"uint64\"||t===\"int64\"?s=f.from(n,BigInt):s=f.from(n)}else if(n instanceof f)s=n;else if(n instanceof Uint8ClampedArray)if(t===\"uint8\")s=Uint8Array.from(n);else throw new TypeError(\"A Uint8ClampedArray tensor's data must be type of uint8\");else if(t===\"float16\"&&n instanceof Uint16Array&&f!==Uint16Array)s=new globalThis.Float16Array(n.buffer,n.byteOffset,n.length);else throw new TypeError(`A ${r} tensor's data must be type of ${f}`)}else if(u=n,Array.isArray(t)){if(t.length===0)throw new TypeError(\"Tensor type cannot be inferred from an empty array.\");let f=typeof t[0];if(f===\"string\")r=\"string\",s=t;else if(f===\"boolean\")r=\"bool\",s=Uint8Array.from(t);else throw new TypeError(`Invalid element type of data array: ${f}.`)}else if(t instanceof Uint8ClampedArray)r=\"uint8\",s=Uint8Array.from(t);else{let f=de.get(t.constructor);if(f===void 0)throw new TypeError(`Unsupported type for tensor data: ${t.constructor}.`);r=f,s=t}if(u===void 0)u=[s.length];else if(!Array.isArray(u))throw new TypeError(\"A tensor's dims must be a number array\");i=u,this.cpuData=s,this.dataLocation=\"cpu\"}let a=_t(i);if(this.cpuData&&a!==this.cpuData.length&&!((r===\"uint4\"||r===\"int4\")&&Math.ceil(a/2)===this.cpuData.length))throw new Error(`Tensor's size(${a}) does not match data length(${this.cpuData.length}).`);this.type=r,this.dims=i,this.size=a}static async fromImage(t,n){return At(t,n)}static fromTexture(t,n){return Ot(t,n)}static fromGpuBuffer(t,n){return It(t,n)}static fromMLTensor(t,n){return Pt(t,n)}static fromPinnedBuffer(t,n,o){return Lt(t,n,o)}toDataURL(t){return Et(this,t)}toImageData(t){return Tt(this,t)}get data(){if(this.ensureValid(),!this.cpuData)throw new Error(\"The data is not on CPU. Use `getData()` to download GPU data to CPU, or use `texture` or `gpuBuffer` property to access the GPU data directly.\");return this.cpuData}get location(){return this.dataLocation}get texture(){if(this.ensureValid(),!this.gpuTextureData)throw new Error(\"The data is not stored as a WebGL texture.\");return this.gpuTextureData}get gpuBuffer(){if(this.ensureValid(),!this.gpuBufferData)throw new Error(\"The data is not stored as a WebGPU buffer.\");return this.gpuBufferData}get mlTensor(){if(this.ensureValid(),!this.mlTensorData)throw new Error(\"The data is not stored as a WebNN MLTensor.\");return this.mlTensorData}async getData(t){switch(this.ensureValid(),this.dataLocation){case\"cpu\":case\"cpu-pinned\":return this.data;case\"texture\":case\"gpu-buffer\":case\"ml-tensor\":{if(!this.downloader)throw new Error(\"The current tensor is not created with a specified data downloader.\");if(this.isDownloading)throw new Error(\"The current tensor is being downloaded.\");try{this.isDownloading=!0;let n=await this.downloader();return this.downloader=void 0,this.dataLocation=\"cpu\",this.cpuData=n,t&&this.disposer&&(this.disposer(),this.disposer=void 0),n}finally{this.isDownloading=!1}}default:throw new Error(`cannot get data from location: ${this.dataLocation}`)}}dispose(){if(this.isDownloading)throw new Error(\"The current tensor is being downloaded.\");this.disposer&&(this.disposer(),this.disposer=void 0),this.cpuData=void 0,this.gpuTextureData=void 0,this.gpuBufferData=void 0,this.mlTensorData=void 0,this.downloader=void 0,this.isDownloading=void 0,this.dataLocation=\"none\"}ensureValid(){if(this.dataLocation===\"none\")throw new Error(\"The tensor is disposed.\")}reshape(t){if(this.ensureValid(),this.downloader||this.disposer)throw new Error(\"Cannot reshape a tensor that owns GPU resource.\");return Mt(this,t)}}});var N,Xe=E(()=>{\"use strict\";Ee();N=U});var Ke,Ct,Y,q,Qe=E(()=>{\"use strict\";Je();Ke=(e,t)=>{(typeof C.trace>\"u\"?!C.wasm.trace:!C.trace)||console.timeStamp(`${e}::ORT::${t}`)},Ct=(e,t)=>{let n=new Error().stack?.split(/\\r\\n|\\r|\\n/g)||[],o=!1;for(let r=0;r<n.length;r++){if(o&&!n[r].includes(\"TRACE_FUNC\")){let i=`FUNC_${e}::${n[r].trim().split(\" \")[1]}`;t&&(i+=`::${t}`),Ke(\"CPU\",i);return}n[r].includes(\"TRACE_FUNC\")&&(o=!0)}},Y=e=>{(typeof C.trace>\"u\"?!C.wasm.trace:!C.trace)||Ct(\"BEGIN\",e)},q=e=>{(typeof C.trace>\"u\"?!C.wasm.trace:!C.trace)||Ct(\"END\",e)}});var Te,Rt=E(()=>{\"use strict\";qe();Xe();Qe();Te=class e{constructor(t){this.handler=t}async run(t,n,o){Y();let r={},i={};if(typeof t!=\"object\"||t===null||t instanceof N||Array.isArray(t))throw new TypeError(\"'feeds' must be an object that use input names as keys and OnnxValue as corresponding values.\");let a=!0;if(typeof n==\"object\"){if(n===null)throw new TypeError(\"Unexpected argument[1]: cannot be null.\");if(n instanceof N)throw new TypeError(\"'fetches' cannot be a Tensor\");if(Array.isArray(n)){if(n.length===0)throw new TypeError(\"'fetches' cannot be an empty array.\");a=!1;for(let f of n){if(typeof f!=\"string\")throw new TypeError(\"'fetches' must be a string array or an object.\");if(this.outputNames.indexOf(f)===-1)throw new RangeError(`'fetches' contains invalid output name: ${f}.`);r[f]=null}if(typeof o==\"object\"&&o!==null)i=o;else if(typeof o<\"u\")throw new TypeError(\"'options' must be an object.\")}else{let f=!1,c=Object.getOwnPropertyNames(n);for(let l of this.outputNames)if(c.indexOf(l)!==-1){let d=n[l];(d===null||d instanceof N)&&(f=!0,a=!1,r[l]=d)}if(f){if(typeof o==\"object\"&&o!==null)i=o;else if(typeof o<\"u\")throw new TypeError(\"'options' must be an object.\")}else i=n}}else if(typeof n<\"u\")throw new TypeError(\"Unexpected argument[1]: must be 'fetches' or 'options'.\");for(let f of this.inputNames)if(typeof t[f]>\"u\")throw new Error(`input '${f}' is missing in 'feeds'.`);if(a)for(let f of this.outputNames)r[f]=null;let s=await this.handler.run(t,r,i),u={};for(let f in s)if(Object.hasOwnProperty.call(s,f)){let c=s[f];c instanceof N?u[f]=c:u[f]=new N(c.type,c.data,c.dims)}return q(),u}async release(){return this.handler.dispose()}static async create(t,n,o,r){Y();let i,a={};if(typeof t==\"string\"){if(i=t,typeof n==\"object\"&&n!==null)a=n;else if(typeof n<\"u\")throw new TypeError(\"'options' must be an object.\")}else if(t instanceof Uint8Array){if(i=t,typeof n==\"object\"&&n!==null)a=n;else if(typeof n<\"u\")throw new TypeError(\"'options' must be an object.\")}else if(t instanceof ArrayBuffer||typeof SharedArrayBuffer<\"u\"&&t instanceof SharedArrayBuffer){let c=t,l=0,d=t.byteLength;if(typeof n==\"object\"&&n!==null)a=n;else if(typeof n==\"number\"){if(l=n,!Number.isSafeInteger(l))throw new RangeError(\"'byteOffset' must be an integer.\");if(l<0||l>=c.byteLength)throw new RangeError(`'byteOffset' is out of range [0, ${c.byteLength}).`);if(d=t.byteLength-l,typeof o==\"number\"){if(d=o,!Number.isSafeInteger(d))throw new RangeError(\"'byteLength' must be an integer.\");if(d<=0||l+d>c.byteLength)throw new RangeError(`'byteLength' is out of range (0, ${c.byteLength-l}].`);if(typeof r==\"object\"&&r!==null)a=r;else if(typeof r<\"u\")throw new TypeError(\"'options' must be an object.\")}else if(typeof o<\"u\")throw new TypeError(\"'byteLength' must be a number.\")}else if(typeof n<\"u\")throw new TypeError(\"'options' must be an object.\");i=new Uint8Array(c,l,d)}else throw new TypeError(\"Unexpected argument[0]: must be 'path' or 'buffer'.\");let[s,u]=await mt(a),f=await s.createInferenceSessionHandler(i,u);return q(),new e(f)}startProfiling(){this.handler.startProfiling()}endProfiling(){this.handler.endProfiling()}get inputNames(){return this.handler.inputNames}get outputNames(){return this.handler.outputNames}get inputMetadata(){return this.handler.inputMetadata}get outputMetadata(){return this.handler.outputMetadata}}});var Ft,kt=E(()=>{\"use strict\";Rt();Ft=Te});var Nt=E(()=>{\"use strict\"});var Wt=E(()=>{\"use strict\"});var Gt=E(()=>{\"use strict\"});var $t=E(()=>{\"use strict\"});var et={};be(et,{InferenceSession:()=>Ft,TRACE:()=>Ke,TRACE_FUNC_BEGIN:()=>Y,TRACE_FUNC_END:()=>q,Tensor:()=>N,env:()=>O,registerBackend:()=>re});var X=E(()=>{\"use strict\";wt();gt();kt();Xe();Nt();Wt();Qe();Gt();$t()});var Se=E(()=>{\"use strict\"});var Vt={};be(Vt,{default:()=>xn});var Ht,jt,xn,Yt=E(()=>{\"use strict\";tt();K();Ae();Ht=\"ort-wasm-proxy-worker\",jt=globalThis.self?.name===Ht;jt&&(self.onmessage=e=>{let{type:t,in:n}=e.data;try{switch(t){case\"init-wasm\":Oe(n.wasm).then(()=>{Ie(n).then(()=>{postMessage({type:t})},o=>{postMessage({type:t,err:o})})},o=>{postMessage({type:t,err:o})});break;case\"init-ep\":{let{epName:o,env:r}=n;Pe(r,o).then(()=>{postMessage({type:t})},i=>{postMessage({type:t,err:i})});break}case\"copy-from\":{let{buffer:o}=n,r=le(o);postMessage({type:t,out:r});break}case\"create\":{let{model:o,options:r}=n;Le(o,r).then(i=>{postMessage({type:t,out:i})},i=>{postMessage({type:t,err:i})});break}case\"release\":xe(n),postMessage({type:t});break;case\"run\":{let{sessionId:o,inputIndices:r,inputs:i,outputIndices:a,options:s}=n;ve(o,r,i,a,new Array(a.length).fill(null),s).then(u=>{u.some(f=>f[3]!==\"cpu\")?postMessage({type:t,err:\"Proxy does not support non-cpu tensor location.\"}):postMessage({type:t,out:u},Ue([...i,...u]))},u=>{postMessage({type:t,err:u})});break}case\"end-profiling\":Be(n),postMessage({type:t});break;default:}}catch(o){postMessage({type:t,err:o})}});xn=jt?null:e=>new Worker(e??R,{type:\"classic\",name:Ht})});var vn,Bn,R,_e,nt,Un,_n,Zt,Mn,qt,Xt,Jt,Kt,Ae=E(()=>{\"use strict\";Se();vn=typeof location>\"u\"?void 0:location.origin,Bn=()=>{if(true)return typeof document<\"u\"?document.currentScript?.src:typeof self<\"u\"?self.location?.href:void 0},R=Bn(),_e=()=>{if(R&&!R.startsWith(\"blob:\"))return R.substring(0,R.lastIndexOf(\"/\")+1)},nt=(e,t)=>{try{let n=t??R;return(n?new URL(e,n):new URL(e)).origin===vn}catch{return!1}},Un=(e,t)=>{let n=t??R;try{return(n?new URL(e,n):new URL(e)).href}catch{return}},_n=(e,t)=>`${t??\"./\"}${e}`,Zt=async e=>{let n=await(await fetch(e,{credentials:\"same-origin\"})).blob();return URL.createObjectURL(n)},Mn=async e=>(await import(/*webpackIgnore:true*/e)).default,qt=(Yt(),Ye(Vt)).default,Xt=async()=>{if(!R)throw new Error(\"Failed to load proxy worker: cannot determine the script source URL.\");if(nt(R))return[void 0,qt()];let e=await Zt(R);return[e,qt(e)]},Jt=void 0,Kt=async(e,t,n)=>{if(!e&&!t&&Jt&&R&&nt(R))return[void 0,Jt];{let o=\"ort-wasm-simd-threaded.mjs\",r=e??Un(o,t),i= true&&n&&r&&!nt(r,t),a=i?await Zt(r):r??_n(o,t);return[i?a:void 0,await Mn(a)]}}});var rt,ot,Me,Qt,Dn,Cn,Rn,Oe,A,K=E(()=>{\"use strict\";Ae();ot=!1,Me=!1,Qt=!1,Dn=()=>{if(typeof SharedArrayBuffer>\"u\")return!1;try{return typeof MessageChannel<\"u\"&&new MessageChannel().port1.postMessage(new SharedArrayBuffer(1)),WebAssembly.validate(new Uint8Array([0,97,115,109,1,0,0,0,1,4,1,96,0,0,3,2,1,0,5,4,1,3,1,1,10,11,1,9,0,65,0,254,16,2,0,26,11]))}catch{return!1}},Cn=()=>{try{return WebAssembly.validate(new Uint8Array([0,97,115,109,1,0,0,0,1,4,1,96,0,0,3,2,1,0,10,30,1,28,0,65,0,253,15,253,12,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,253,186,1,26,11]))}catch{return!1}},Rn=()=>{try{return WebAssembly.validate(new Uint8Array([0,97,115,109,1,0,0,0,1,5,1,96,0,1,123,3,2,1,0,10,19,1,17,0,65,1,253,15,65,2,253,15,65,3,253,15,253,147,2,11]))}catch{return!1}},Oe=async e=>{if(ot)return Promise.resolve();if(Me)throw new Error(\"multiple calls to 'initializeWebAssembly()' detected.\");if(Qt)throw new Error(\"previous call to 'initializeWebAssembly()' failed.\");Me=!0;let t=e.initTimeout,n=e.numThreads;if(e.simd!==!1){if(e.simd===\"relaxed\"){if(!Rn())throw new Error(\"Relaxed WebAssembly SIMD is not supported in the current environment.\")}else if(!Cn())throw new Error(\"WebAssembly SIMD is not supported in the current environment.\")}let o=Dn();n>1&&!o&&(typeof self<\"u\"&&!self.crossOriginIsolated&&console.warn(\"env.wasm.numThreads is set to \"+n+\", but this will not work unless you enable crossOriginIsolated mode. See https://web.dev/cross-origin-isolation-guide/ for more info.\"),console.warn(\"WebAssembly multi-threading is not supported in the current environment. Falling back to single-threading.\"),e.numThreads=n=1);let r=e.wasmPaths,i=typeof r==\"string\"?r:void 0,a=r?.mjs,s=a?.href??a,u=r?.wasm,f=u?.href??u,c=e.wasmBinary,[l,d]=await Kt(s,i,n>1),p=!1,w=[];if(t>0&&w.push(new Promise(b=>{setTimeout(()=>{p=!0,b()},t)})),w.push(new Promise((b,I)=>{let m={numThreads:n};if(c)m.wasmBinary=c;else if(f||i)m.locateFile=h=>f??i+h;else if(s&&s.indexOf(\"blob:\")!==0)m.locateFile=h=>new URL(h,s).href;else if(l){let h=_e();h&&(m.locateFile=P=>h+P)}d(m).then(h=>{Me=!1,ot=!0,rt=h,b(),l&&URL.revokeObjectURL(l)},h=>{Me=!1,Qt=!0,I(h)})})),await Promise.race(w),p)throw new Error(`WebAssembly backend initializing failed due to timeout: ${t}ms`)},A=()=>{if(ot&&rt)return rt;throw new Error(\"WebAssembly is not initialized yet.\")}});var F,pe,T,De=E(()=>{\"use strict\";K();F=(e,t)=>{let n=A(),o=n.lengthBytesUTF8(e)+1,r=n._malloc(o);return n.stringToUTF8(e,r,o),t.push(r),r},pe=(e,t,n,o)=>{if(typeof e==\"object\"&&e!==null){if(n.has(e))throw new Error(\"Circular reference in options\");n.add(e)}Object.entries(e).forEach(([r,i])=>{let a=t?t+r:r;if(typeof i==\"object\")pe(i,a+\".\",n,o);else if(typeof i==\"string\"||typeof i==\"number\")o(a,i.toString());else if(typeof i==\"boolean\")o(a,i?\"1\":\"0\");else throw new Error(`Can't handle extra config type: ${typeof i}`)})},T=e=>{let t=A(),n=t.stackSave();try{let o=t.PTR_SIZE,r=t.stackAlloc(2*o);t._OrtGetLastError(r,r+o);let i=Number(t.getValue(r,o===4?\"i32\":\"i64\")),a=t.getValue(r+o,\"*\"),s=a?t.UTF8ToString(a):\"\";throw new Error(`${e} ERROR_CODE: ${i}, ERROR_MESSAGE: ${s}`)}finally{t.stackRestore(n)}}});var en,tn=E(()=>{\"use strict\";K();De();en=e=>{let t=A(),n=0,o=[],r=e||{};try{if(e?.logSeverityLevel===void 0)r.logSeverityLevel=2;else if(typeof e.logSeverityLevel!=\"number\"||!Number.isInteger(e.logSeverityLevel)||e.logSeverityLevel<0||e.logSeverityLevel>4)throw new Error(`log serverity level is not valid: ${e.logSeverityLevel}`);if(e?.logVerbosityLevel===void 0)r.logVerbosityLevel=0;else if(typeof e.logVerbosityLevel!=\"number\"||!Number.isInteger(e.logVerbosityLevel))throw new Error(`log verbosity level is not valid: ${e.logVerbosityLevel}`);e?.terminate===void 0&&(r.terminate=!1);let i=0;return e?.tag!==void 0&&(i=F(e.tag,o)),n=t._OrtCreateRunOptions(r.logSeverityLevel,r.logVerbosityLevel,!!r.terminate,i),n===0&&T(\"Can't create run options.\"),e?.extra!==void 0&&pe(e.extra,\"\",new WeakSet,(a,s)=>{let u=F(a,o),f=F(s,o);t._OrtAddRunConfigEntry(n,u,f)!==0&&T(`Can't set a run config entry: ${a} - ${s}.`)}),[n,o]}catch(i){throw n!==0&&t._OrtReleaseRunOptions(n),o.forEach(a=>t._free(a)),i}}});var Fn,kn,Nn,Ce,Wn,nn,rn=E(()=>{\"use strict\";K();De();Fn=e=>{switch(e){case\"disabled\":return 0;case\"basic\":return 1;case\"extended\":return 2;case\"all\":return 99;default:throw new Error(`unsupported graph optimization level: ${e}`)}},kn=e=>{switch(e){case\"sequential\":return 0;case\"parallel\":return 1;default:throw new Error(`unsupported execution mode: ${e}`)}},Nn=e=>{e.extra||(e.extra={}),e.extra.session||(e.extra.session={});let t=e.extra.session;t.use_ort_model_bytes_directly||(t.use_ort_model_bytes_directly=\"1\"),e.executionProviders&&e.executionProviders.some(n=>(typeof n==\"string\"?n:n.name)===\"webgpu\")&&(e.enableMemPattern=!1)},Ce=(e,t,n,o)=>{let r=F(t,o),i=F(n,o);A()._OrtAddSessionConfigEntry(e,r,i)!==0&&T(`Can't set a session config entry: ${t} - ${n}.`)},Wn=async(e,t,n)=>{for(let o of t){let r=typeof o==\"string\"?o:o.name,i=[];switch(r){case\"webnn\":if(r=\"WEBNN\",typeof o!=\"string\"){let l=o?.deviceType;l&&Ce(e,\"deviceType\",l,n)}break;case\"webgpu\":if(r=\"JS\",typeof o!=\"string\"){let c=o;if(c?.preferredLayout){if(c.preferredLayout!==\"NCHW\"&&c.preferredLayout!==\"NHWC\")throw new Error(`preferredLayout must be either 'NCHW' or 'NHWC': ${c.preferredLayout}`);Ce(e,\"preferredLayout\",c.preferredLayout,n)}}break;case\"wasm\":case\"cpu\":continue;default:throw new Error(`not supported execution provider: ${r}`)}let a=F(r,n),s=i.length,u=0,f=0;if(s>0){u=A()._malloc(s*A().PTR_SIZE),n.push(u),f=A()._malloc(s*A().PTR_SIZE),n.push(f);for(let c=0;c<s;c++)A().setValue(u+c*A().PTR_SIZE,i[c][0],\"*\"),A().setValue(f+c*A().PTR_SIZE,i[c][1],\"*\")}await A()._OrtAppendExecutionProvider(e,a,u,f,s)!==0&&T(`Can't append execution provider: ${r}.`)}},nn=async e=>{let t=A(),n=0,o=[],r=e||{};Nn(r);try{let i=Fn(r.graphOptimizationLevel??\"all\"),a=kn(r.executionMode??\"sequential\"),s=typeof r.logId==\"string\"?F(r.logId,o):0,u=r.logSeverityLevel??2;if(!Number.isInteger(u)||u<0||u>4)throw new Error(`log serverity level is not valid: ${u}`);let f=r.logVerbosityLevel??0;if(!Number.isInteger(f)||f<0||f>4)throw new Error(`log verbosity level is not valid: ${f}`);let c=typeof r.optimizedModelFilePath==\"string\"?F(r.optimizedModelFilePath,o):0;if(n=t._OrtCreateSessionOptions(i,!!r.enableCpuMemArena,!!r.enableMemPattern,a,!!r.enableProfiling,0,s,u,f,c),n===0&&T(\"Can't create session options.\"),r.executionProviders&&await Wn(n,r.executionProviders,o),r.enableGraphCapture!==void 0){if(typeof r.enableGraphCapture!=\"boolean\")throw new Error(`enableGraphCapture must be a boolean value: ${r.enableGraphCapture}`);Ce(n,\"enableGraphCapture\",r.enableGraphCapture.toString(),o)}if(r.freeDimensionOverrides)for(let[l,d]of Object.entries(r.freeDimensionOverrides)){if(typeof l!=\"string\")throw new Error(`free dimension override name must be a string: ${l}`);if(typeof d!=\"number\"||!Number.isInteger(d)||d<0)throw new Error(`free dimension override value must be a non-negative integer: ${d}`);let p=F(l,o);t._OrtAddFreeDimensionOverride(n,p,d)!==0&&T(`Can't set a free dimension override: ${l} - ${d}.`)}return r.extra!==void 0&&pe(r.extra,\"\",new WeakSet,(l,d)=>{Ce(n,l,d,o)}),[n,o]}catch(i){throw n!==0&&t._OrtReleaseSessionOptions(n)!==0&&T(\"Can't release session options.\"),o.forEach(a=>t._free(a)),i}}});var oe,Re,se,on,sn,Fe,ke,an,st=E(()=>{\"use strict\";oe=e=>{switch(e){case\"int8\":return 3;case\"uint8\":return 2;case\"bool\":return 9;case\"int16\":return 5;case\"uint16\":return 4;case\"int32\":return 6;case\"uint32\":return 12;case\"float16\":return 10;case\"float32\":return 1;case\"float64\":return 11;case\"string\":return 8;case\"int64\":return 7;case\"uint64\":return 13;case\"int4\":return 22;case\"uint4\":return 21;default:throw new Error(`unsupported data type: ${e}`)}},Re=e=>{switch(e){case 3:return\"int8\";case 2:return\"uint8\";case 9:return\"bool\";case 5:return\"int16\";case 4:return\"uint16\";case 6:return\"int32\";case 12:return\"uint32\";case 10:return\"float16\";case 1:return\"float32\";case 11:return\"float64\";case 8:return\"string\";case 7:return\"int64\";case 13:return\"uint64\";case 22:return\"int4\";case 21:return\"uint4\";default:throw new Error(`unsupported data type: ${e}`)}},se=(e,t)=>{let n=[-1,4,1,1,2,2,4,8,-1,1,2,8,4,8,-1,-1,-1,-1,-1,-1,-1,.5,.5][e],o=typeof t==\"number\"?t:t.reduce((r,i)=>r*i,1);return n>0?Math.ceil(o*n):void 0},on=e=>{switch(e){case\"float16\":return typeof Float16Array<\"u\"&&Float16Array.from?Float16Array:Uint16Array;case\"float32\":return Float32Array;case\"uint8\":return Uint8Array;case\"int8\":return Int8Array;case\"uint16\":return Uint16Array;case\"int16\":return Int16Array;case\"int32\":return Int32Array;case\"bool\":return Uint8Array;case\"float64\":return Float64Array;case\"uint32\":return Uint32Array;case\"int64\":return BigInt64Array;case\"uint64\":return BigUint64Array;default:throw new Error(`unsupported type: ${e}`)}},sn=e=>{switch(e){case\"verbose\":return 0;case\"info\":return 1;case\"warning\":return 2;case\"error\":return 3;case\"fatal\":return 4;default:throw new Error(`unsupported logging level: ${e}`)}},Fe=e=>e===\"float32\"||e===\"float16\"||e===\"int32\"||e===\"int64\"||e===\"uint32\"||e===\"uint8\"||e===\"bool\"||e===\"uint4\"||e===\"int4\",ke=e=>e===\"float32\"||e===\"float16\"||e===\"int32\"||e===\"int64\"||e===\"uint32\"||e===\"uint64\"||e===\"int8\"||e===\"uint8\"||e===\"bool\"||e===\"uint4\"||e===\"int4\",an=e=>{switch(e){case\"none\":return 0;case\"cpu\":return 1;case\"cpu-pinned\":return 2;case\"texture\":return 3;case\"gpu-buffer\":return 4;case\"ml-tensor\":return 5;default:throw new Error(`unsupported data location: ${e}`)}}});var me,at=E(()=>{\"use strict\";Se();me=async e=>{if(typeof e==\"string\")if(false){}else{let t=await fetch(e);if(!t.ok)throw new Error(`failed to load external data file: ${e}`);let n=t.headers.get(\"Content-Length\"),o=n?parseInt(n,10):0;if(o<1073741824)return new Uint8Array(await t.arrayBuffer());{if(!t.body)throw new Error(`failed to load external data file: ${e}, no response body.`);let r=t.body.getReader(),i;try{i=new ArrayBuffer(o)}catch(s){if(s instanceof RangeError){let u=Math.ceil(o/65536);i=new WebAssembly.Memory({initial:u,maximum:u}).buffer}else throw s}let a=0;for(;;){let{done:s,value:u}=await r.read();if(s)break;let f=u.byteLength;new Uint8Array(i,a,f).set(u),a+=f}return new Uint8Array(i,0,o)}}else return e instanceof Blob?new Uint8Array(await e.arrayBuffer()):e instanceof Uint8Array?e:new Uint8Array(e)}});var Gn,Ie,Pe,ae,$n,un,le,Le,xe,fn,ve,Be,Ue,tt=E(()=>{\"use strict\";tn();rn();st();K();De();at();Gn=(e,t)=>{A()._OrtInit(e,t)!==0&&T(\"Can't initialize onnxruntime.\")},Ie=async e=>{Gn(e.wasm.numThreads,sn(e.logLevel))},Pe=async(e,t)=>{A().asyncInit?.()},ae=new Map,$n=e=>{let t=A(),n=t.stackSave();try{let o=t.PTR_SIZE,r=t.stackAlloc(2*o);t._OrtGetInputOutputCount(e,r,r+o)!==0&&T(\"Can't get session input/output count.\");let a=o===4?\"i32\":\"i64\";return[Number(t.getValue(r,a)),Number(t.getValue(r+o,a))]}finally{t.stackRestore(n)}},un=(e,t)=>{let n=A(),o=n.stackSave(),r=0;try{let i=n.PTR_SIZE,a=n.stackAlloc(2*i);n._OrtGetInputOutputMetadata(e,t,a,a+i)!==0&&T(\"Can't get session input/output metadata.\");let u=Number(n.getValue(a,\"*\"));r=Number(n.getValue(a+i,\"*\"));let f=n.HEAP32[r/4];if(f===0)return[u,0];let c=n.HEAPU32[r/4+1],l=[];for(let d=0;d<c;d++){let p=Number(n.getValue(r+8+d*i,\"*\"));l.push(p!==0?n.UTF8ToString(p):Number(n.getValue(r+8+(d+c)*i,\"*\")))}return[u,f,l]}finally{n.stackRestore(o),r!==0&&n._OrtFree(r)}},le=e=>{let t=A(),n=t._malloc(e.byteLength);if(n===0)throw new Error(`Can't create a session. failed to allocate a buffer of size ${e.byteLength}.`);return t.HEAPU8.set(e,n),[n,e.byteLength]},Le=async(e,t)=>{let n,o,r=A();Array.isArray(e)?[n,o]=e:e.buffer===r.HEAPU8.buffer?[n,o]=[e.byteOffset,e.byteLength]:[n,o]=le(e);let i=0,a=0,s=0,u=[],f=[],c=[];try{if([a,u]=await nn(t),t?.externalData&&r.mountExternalData){let g=[];for(let S of t.externalData){let B=typeof S==\"string\"?S:S.path;g.push(me(typeof S==\"string\"?S:S.data).then(D=>{r.mountExternalData(B,D)}))}await Promise.all(g)}for(let g of t?.executionProviders??[])if((typeof g==\"string\"?g:g.name)===\"webnn\"){if(r.shouldTransferToMLTensor=!1,typeof g!=\"string\"){let B=g,D=B?.context,_=B?.gpuDevice,te=B?.deviceType,fe=B?.powerPreference;D?r.currentContext=D:_?r.currentContext=await r.webnnCreateMLContext(_):r.currentContext=await r.webnnCreateMLContext({deviceType:te,powerPreference:fe})}else r.currentContext=await r.webnnCreateMLContext();break}i=await r._OrtCreateSession(n,o,a),r.webgpuOnCreateSession?.(i),i===0&&T(\"Can't create a session.\"),r.jsepOnCreateSession?.(),r.currentContext&&(r.webnnRegisterMLContext(i,r.currentContext),r.currentContext=void 0,r.shouldTransferToMLTensor=!0);let[l,d]=$n(i),p=!!t?.enableGraphCapture,w=[],b=[],I=[],m=[],h=[];for(let g=0;g<l;g++){let[S,B,D]=un(i,g);S===0&&T(\"Can't get an input name.\"),f.push(S);let _=r.UTF8ToString(S);w.push(_),I.push(B===0?{name:_,isTensor:!1}:{name:_,isTensor:!0,type:Re(B),shape:D})}for(let g=0;g<d;g++){let[S,B,D]=un(i,g+l);S===0&&T(\"Can't get an output name.\"),c.push(S);let _=r.UTF8ToString(S);b.push(_),m.push(B===0?{name:_,isTensor:!1}:{name:_,isTensor:!0,type:Re(B),shape:D})}return ae.set(i,[i,f,c,null,p,!1]),[i,w,b,I,m]}catch(l){throw f.forEach(d=>r._OrtFree(d)),c.forEach(d=>r._OrtFree(d)),s!==0&&r._OrtReleaseBinding(s)!==0&&T(\"Can't release IO binding.\"),i!==0&&r._OrtReleaseSession(i)!==0&&T(\"Can't release session.\"),l}finally{r._free(n),a!==0&&r._OrtReleaseSessionOptions(a)!==0&&T(\"Can't release session options.\"),u.forEach(l=>r._free(l)),r.unmountExternalData?.()}},xe=e=>{let t=A(),n=ae.get(e);if(!n)throw new Error(`cannot release session. invalid session id: ${e}`);let[o,r,i,a,s]=n;a&&(s&&t._OrtClearBoundOutputs(a.handle)!==0&&T(\"Can't clear bound outputs.\"),t._OrtReleaseBinding(a.handle)!==0&&T(\"Can't release IO binding.\")),t.jsepOnReleaseSession?.(e),t.webnnOnReleaseSession?.(e),t.webgpuOnReleaseSession?.(e),r.forEach(u=>t._OrtFree(u)),i.forEach(u=>t._OrtFree(u)),t._OrtReleaseSession(o)!==0&&T(\"Can't release session.\"),ae.delete(e)},fn=async(e,t,n,o,r,i,a=!1)=>{if(!e){t.push(0);return}let s=A(),u=s.PTR_SIZE,f=e[0],c=e[1],l=e[3],d=l,p,w;if(f===\"string\"&&(l===\"gpu-buffer\"||l===\"ml-tensor\"))throw new Error(\"String tensor is not supported on GPU.\");if(a&&l!==\"gpu-buffer\")throw new Error(`External buffer must be provided for input/output index ${i} when enableGraphCapture is true.`);if(l===\"gpu-buffer\"){let m=e[2].gpuBuffer;w=se(oe(f),c);{let h=s.jsepRegisterBuffer;if(!h)throw new Error('Tensor location \"gpu-buffer\" is not supported without using WebGPU.');p=h(o,i,m,w)}}else if(l===\"ml-tensor\"){let m=e[2].mlTensor;w=se(oe(f),c);let h=s.webnnRegisterMLTensor;if(!h)throw new Error('Tensor location \"ml-tensor\" is not supported without using WebNN.');p=h(o,m,oe(f),c)}else{let m=e[2];if(Array.isArray(m)){w=u*m.length,p=s._malloc(w),n.push(p);for(let h=0;h<m.length;h++){if(typeof m[h]!=\"string\")throw new TypeError(`tensor data at index ${h} is not a string`);s.setValue(p+h*u,F(m[h],n),\"*\")}}else{let h=s.webnnIsGraphInput,P=s.webnnIsGraphOutput;if(f!==\"string\"&&h&&P){let g=s.UTF8ToString(r);if(h(o,g)||P(o,g)){let S=oe(f);w=se(S,c),d=\"ml-tensor\";let B=s.webnnCreateTemporaryTensor,D=s.webnnUploadTensor;if(!B||!D)throw new Error('Tensor location \"ml-tensor\" is not supported without using WebNN.');let _=await B(o,S,c);D(_,new Uint8Array(m.buffer,m.byteOffset,m.byteLength)),p=_}else w=m.byteLength,p=s._malloc(w),n.push(p),s.HEAPU8.set(new Uint8Array(m.buffer,m.byteOffset,w),p)}else w=m.byteLength,p=s._malloc(w),n.push(p),s.HEAPU8.set(new Uint8Array(m.buffer,m.byteOffset,w),p)}}let b=s.stackSave(),I=s.stackAlloc(4*c.length);try{c.forEach((h,P)=>s.setValue(I+P*u,h,u===4?\"i32\":\"i64\"));let m=s._OrtCreateTensor(oe(f),p,w,I,c.length,an(d));m===0&&T(`Can't create tensor for input/output. session=${o}, index=${i}.`),t.push(m)}finally{s.stackRestore(b)}},ve=async(e,t,n,o,r,i)=>{let a=A(),s=a.PTR_SIZE,u=ae.get(e);if(!u)throw new Error(`cannot run inference. invalid session id: ${e}`);let f=u[0],c=u[1],l=u[2],d=u[3],p=u[4],w=u[5],b=t.length,I=o.length,m=0,h=[],P=[],g=[],S=[],B=a.stackSave(),D=a.stackAlloc(b*s),_=a.stackAlloc(b*s),te=a.stackAlloc(I*s),fe=a.stackAlloc(I*s);try{[m,h]=en(i);for(let y=0;y<b;y++)await fn(n[y],P,S,e,c[t[y]],t[y],p);for(let y=0;y<I;y++)await fn(r[y],g,S,e,l[o[y]],b+o[y],p);for(let y=0;y<b;y++)a.setValue(D+y*s,P[y],\"*\"),a.setValue(_+y*s,c[t[y]],\"*\");for(let y=0;y<I;y++)a.setValue(te+y*s,g[y],\"*\"),a.setValue(fe+y*s,l[o[y]],\"*\");a.jsepOnRunStart?.(f),a.webnnOnRunStart?.(f);let k;k=await a._OrtRun(f,_,D,b,fe,I,te,m),k!==0&&T(\"failed to call OrtRun().\");let $=[],ct=[];for(let y=0;y<I;y++){let z=Number(a.getValue(te+y*s,\"*\"));if(z===g[y]){$.push(r[y]);continue}let dt=a.stackSave(),G=a.stackAlloc(4*s),ne=!1,x,M=0;try{a._OrtGetTensorData(z,G,G+s,G+2*s,G+3*s)!==0&&T(`Can't access output tensor data on index ${y}.`);let He=s===4?\"i32\":\"i64\",he=Number(a.getValue(G,He));M=a.getValue(G+s,\"*\");let lt=a.getValue(G+s*2,\"*\"),Sn=Number(a.getValue(G+s*3,He)),H=[];for(let v=0;v<Sn;v++)H.push(Number(a.getValue(lt+v*s,He)));a._OrtFree(lt)!==0&&T(\"Can't free memory for tensor dims.\");let j=H.reduce((v,L)=>v*L,1);x=Re(he);let ce=d?.outputPreferredLocations[o[y]];if(x===\"string\"){if(ce===\"gpu-buffer\"||ce===\"ml-tensor\")throw new Error(\"String tensor is not supported on GPU.\");let v=[];for(let L=0;L<j;L++){let V=a.getValue(M+L*s,\"*\"),ye=a.getValue(M+(L+1)*s,\"*\"),pt=L===j-1?void 0:ye-V;v.push(a.UTF8ToString(V,pt))}$.push([x,H,v,\"cpu\"])}else if(ce===\"gpu-buffer\"&&j>0){let v=a.jsepGetBuffer;if(!v)throw new Error('preferredLocation \"gpu-buffer\" is not supported without using WebGPU.');let L=v(M),V=se(he,j);if(V===void 0||!Fe(x))throw new Error(`Unsupported data type: ${x}`);ne=!0,$.push([x,H,{gpuBuffer:L,download:a.jsepCreateDownloader(L,V,x),dispose:()=>{a._OrtReleaseTensor(z)!==0&&T(\"Can't release tensor.\")}},\"gpu-buffer\"])}else if(ce===\"ml-tensor\"&&j>0){let v=a.webnnEnsureTensor,L=a.webnnIsGraphInputOutputTypeSupported;if(!v||!L)throw new Error('preferredLocation \"ml-tensor\" is not supported without using WebNN.');if(se(he,j)===void 0||!ke(x))throw new Error(`Unsupported data type: ${x}`);if(!L(e,x,!1))throw new Error(`preferredLocation \"ml-tensor\" for ${x} output is not supported by current WebNN Context.`);let ye=await v(e,M,he,H,!1);ne=!0,$.push([x,H,{mlTensor:ye,download:a.webnnCreateMLTensorDownloader(M,x),dispose:()=>{a.webnnReleaseTensorId(M),a._OrtReleaseTensor(z)}},\"ml-tensor\"])}else if(ce===\"ml-tensor-cpu-output\"&&j>0){let v=a.webnnCreateMLTensorDownloader(M,x)(),L=$.length;ne=!0,ct.push((async()=>{let V=[L,await v];return a.webnnReleaseTensorId(M),a._OrtReleaseTensor(z),V})()),$.push([x,H,[],\"cpu\"])}else{let v=on(x),L=new v(j);new Uint8Array(L.buffer,L.byteOffset,L.byteLength).set(a.HEAPU8.subarray(M,M+L.byteLength)),$.push([x,H,L,\"cpu\"])}}finally{a.stackRestore(dt),x===\"string\"&&M&&a._free(M),ne||a._OrtReleaseTensor(z)}}d&&!p&&(a._OrtClearBoundOutputs(d.handle)!==0&&T(\"Can't clear bound outputs.\"),ae.set(e,[f,c,l,d,p,!1]));for(let[y,z]of await Promise.all(ct))$[y][2]=z;return $}finally{a.webnnOnRunEnd?.(f),a.stackRestore(B),P.forEach(k=>a._OrtReleaseTensor(k)),g.forEach(k=>a._OrtReleaseTensor(k)),S.forEach(k=>a._free(k)),m!==0&&a._OrtReleaseRunOptions(m),h.forEach(k=>a._free(k))}},Be=e=>{let t=A(),n=ae.get(e);if(!n)throw new Error(\"invalid session id\");let o=n[0],r=t._OrtEndProfiling(o);r===0&&T(\"Can't get an profile file name.\"),t._OrtFree(r)},Ue=e=>{let t=[];for(let n of e){let o=n[2];!Array.isArray(o)&&\"buffer\"in o&&t.push(o.buffer)}return t}});var ee,W,we,We,Ge,Ne,it,ut,ie,ue,Hn,cn,dn,ln,pn,mn,wn,hn,ft=E(()=>{\"use strict\";X();tt();K();Ae();ee=()=>!!O.wasm.proxy&&typeof document<\"u\",we=!1,We=!1,Ge=!1,ut=new Map,ie=(e,t)=>{let n=ut.get(e);n?n.push(t):ut.set(e,[t])},ue=()=>{if(we||!We||Ge||!W)throw new Error(\"worker not ready\")},Hn=e=>{switch(e.data.type){case\"init-wasm\":we=!1,e.data.err?(Ge=!0,it[1](e.data.err)):(We=!0,it[0]()),Ne&&(URL.revokeObjectURL(Ne),Ne=void 0);break;case\"init-ep\":case\"copy-from\":case\"create\":case\"release\":case\"run\":case\"end-profiling\":{let t=ut.get(e.data.type);e.data.err?t.shift()[1](e.data.err):t.shift()[0](e.data.out);break}default:}},cn=async()=>{if(!We){if(we)throw new Error(\"multiple calls to 'initWasm()' detected.\");if(Ge)throw new Error(\"previous call to 'initWasm()' failed.\");if(we=!0,ee())return new Promise((e,t)=>{W?.terminate(),Xt().then(([n,o])=>{try{W=o,W.onerror=i=>t(i),W.onmessage=Hn,it=[e,t];let r={type:\"init-wasm\",in:O};if(!r.in.wasm.wasmPaths&&n){let i=_e();i&&(r.in.wasm.wasmPaths=i)}W.postMessage(r),Ne=n}catch(r){t(r)}},t)});try{await Oe(O.wasm),await Ie(O),We=!0}catch(e){throw Ge=!0,e}finally{we=!1}}},dn=async e=>{if(ee())return ue(),new Promise((t,n)=>{ie(\"init-ep\",[t,n]);let o={type:\"init-ep\",in:{epName:e,env:O}};W.postMessage(o)});await Pe(O,e)},ln=async e=>ee()?(ue(),new Promise((t,n)=>{ie(\"copy-from\",[t,n]);let o={type:\"copy-from\",in:{buffer:e}};W.postMessage(o,[e.buffer])})):le(e),pn=async(e,t)=>{if(ee()){if(t?.preferredOutputLocation)throw new Error('session option \"preferredOutputLocation\" is not supported for proxy.');return ue(),new Promise((n,o)=>{ie(\"create\",[n,o]);let r={type:\"create\",in:{model:e,options:{...t}}},i=[];e instanceof Uint8Array&&i.push(e.buffer),W.postMessage(r,i)})}else return Le(e,t)},mn=async e=>{if(ee())return ue(),new Promise((t,n)=>{ie(\"release\",[t,n]);let o={type:\"release\",in:e};W.postMessage(o)});xe(e)},wn=async(e,t,n,o,r,i)=>{if(ee()){if(n.some(a=>a[3]!==\"cpu\"))throw new Error(\"input tensor on GPU is not supported for proxy.\");if(r.some(a=>a))throw new Error(\"pre-allocated output tensor is not supported for proxy.\");return ue(),new Promise((a,s)=>{ie(\"run\",[a,s]);let u=n,f={type:\"run\",in:{sessionId:e,inputIndices:t,inputs:u,outputIndices:o,options:i}};W.postMessage(f,Ue(u))})}else return ve(e,t,n,o,r,i)},hn=async e=>{if(ee())return ue(),new Promise((t,n)=>{ie(\"end-profiling\",[t,n]);let o={type:\"end-profiling\",in:e};W.postMessage(o)});Be(e)}});var yn,jn,$e,bn=E(()=>{\"use strict\";X();ft();st();Se();at();yn=(e,t)=>{switch(e.location){case\"cpu\":return[e.type,e.dims,e.data,\"cpu\"];case\"gpu-buffer\":return[e.type,e.dims,{gpuBuffer:e.gpuBuffer},\"gpu-buffer\"];case\"ml-tensor\":return[e.type,e.dims,{mlTensor:e.mlTensor},\"ml-tensor\"];default:throw new Error(`invalid data location: ${e.location} for ${t()}`)}},jn=e=>{switch(e[3]){case\"cpu\":return new N(e[0],e[2],e[1]);case\"gpu-buffer\":{let t=e[0];if(!Fe(t))throw new Error(`not supported data type: ${t} for deserializing GPU tensor`);let{gpuBuffer:n,download:o,dispose:r}=e[2];return N.fromGpuBuffer(n,{dataType:t,dims:e[1],download:o,dispose:r})}case\"ml-tensor\":{let t=e[0];if(!ke(t))throw new Error(`not supported data type: ${t} for deserializing MLTensor tensor`);let{mlTensor:n,download:o,dispose:r}=e[2];return N.fromMLTensor(n,{dataType:t,dims:e[1],download:o,dispose:r})}default:throw new Error(`invalid data location: ${e[3]}`)}},$e=class{async fetchModelAndCopyToWasmMemory(t){return ln(await me(t))}async loadModel(t,n){Y();let o;typeof t==\"string\"?o=await this.fetchModelAndCopyToWasmMemory(t):o=t,[this.sessionId,this.inputNames,this.outputNames,this.inputMetadata,this.outputMetadata]=await pn(o,n),q()}async dispose(){return mn(this.sessionId)}async run(t,n,o){Y();let r=[],i=[];Object.entries(t).forEach(d=>{let p=d[0],w=d[1],b=this.inputNames.indexOf(p);if(b===-1)throw new Error(`invalid input '${p}'`);r.push(w),i.push(b)});let a=[],s=[];Object.entries(n).forEach(d=>{let p=d[0],w=d[1],b=this.outputNames.indexOf(p);if(b===-1)throw new Error(`invalid output '${p}'`);a.push(w),s.push(b)});let u=r.map((d,p)=>yn(d,()=>`input \"${this.inputNames[i[p]]}\"`)),f=a.map((d,p)=>d?yn(d,()=>`output \"${this.outputNames[s[p]]}\"`):null),c=await wn(this.sessionId,i,u,s,f,o),l={};for(let d=0;d<c.length;d++)l[this.outputNames[s[d]]]=a[d]??jn(c[d]);return q(),l}startProfiling(){}endProfiling(){hn(this.sessionId)}}});var En={};be(En,{OnnxruntimeWebAssemblyBackend:()=>ze,initializeFlags:()=>gn,wasmBackend:()=>Vn});var gn,ze,Vn,Tn=E(()=>{\"use strict\";X();ft();bn();gn=()=>{(typeof O.wasm.initTimeout!=\"number\"||O.wasm.initTimeout<0)&&(O.wasm.initTimeout=0);let e=O.wasm.simd;if(typeof e!=\"boolean\"&&e!==void 0&&e!==\"fixed\"&&e!==\"relaxed\"&&(console.warn(`Property \"env.wasm.simd\" is set to unknown value \"${e}\". Reset it to \\`false\\` and ignore SIMD feature checking.`),O.wasm.simd=!1),typeof O.wasm.proxy!=\"boolean\"&&(O.wasm.proxy=!1),typeof O.wasm.trace!=\"boolean\"&&(O.wasm.trace=!1),typeof O.wasm.numThreads!=\"number\"||!Number.isInteger(O.wasm.numThreads)||O.wasm.numThreads<=0)if(typeof self<\"u\"&&!self.crossOriginIsolated)O.wasm.numThreads=1;else{let t=typeof navigator>\"u\"?Ve(\"node:os\").cpus().length:navigator.hardwareConcurrency;O.wasm.numThreads=Math.min(4,Math.ceil((t||1)/2))}},ze=class{async init(t){gn(),await cn(),await dn(t)}async createInferenceSessionHandler(t,n){let o=new $e;return await o.loadModel(t,n),o}},Vn=new ze});var qn={};be(qn,{InferenceSession:()=>Ft,TRACE:()=>Ke,TRACE_FUNC_BEGIN:()=>Y,TRACE_FUNC_END:()=>q,Tensor:()=>N,default:()=>Yn,env:()=>O,registerBackend:()=>re});X();X();X();var zt=\"1.22.0\";var Yn=et;{let e=(Tn(),Ye(En)).wasmBackend;re(\"cpu\",e,10),re(\"wasm\",e,10)}Object.defineProperty(O.versions,\"web\",{value:zt,enumerable:!0});return Ye(qn);})();\n true&&(module.exports=ort);\n//# sourceMappingURL=ort.wasm.min.js.map\n\n\n//# sourceURL=webpack://vad/../../node_modules/onnxruntime-web/dist/ort.wasm.min.js?");

/***/ }),

/***/ "../../node_modules/onnxruntime-web/dist sync recursive":
/*!*****************************************************!*\
  !*** ../../node_modules/onnxruntime-web/dist/ sync ***!
  \*****************************************************/
/***/ ((module) => {

eval("function webpackEmptyContext(req) {\n\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\te.code = 'MODULE_NOT_FOUND';\n\tthrow e;\n}\nwebpackEmptyContext.keys = () => ([]);\nwebpackEmptyContext.resolve = webpackEmptyContext;\nwebpackEmptyContext.id = \"../../node_modules/onnxruntime-web/dist sync recursive\";\nmodule.exports = webpackEmptyContext;\n\n//# sourceURL=webpack://vad/../../node_modules/onnxruntime-web/dist/_sync?");

/***/ }),

/***/ "./dist/asset-path.js":
/*!****************************!*\
  !*** ./dist/asset-path.js ***!
  \****************************/
/***/ ((__unused_webpack_module, exports) => {

"use strict";
eval("\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.baseAssetPath = void 0;\n// nextjs@14 bundler may attempt to execute this during SSR and crash\nconst isWeb = typeof window !== \"undefined\" && typeof window.document !== \"undefined\";\nconst currentScript = isWeb\n    ? window.document.currentScript\n    : null;\nlet basePath = \"/\";\nif (currentScript) {\n    basePath = currentScript.src\n        .replace(/#.*$/, \"\")\n        .replace(/\\?.*$/, \"\")\n        .replace(/\\/[^/]+$/, \"/\");\n}\nexports.baseAssetPath = basePath;\n//# sourceMappingURL=asset-path.js.map\n\n//# sourceURL=webpack://vad/./dist/asset-path.js?");

/***/ }),

/***/ "./dist/default-model-fetcher.js":
/*!***************************************!*\
  !*** ./dist/default-model-fetcher.js ***!
  \***************************************/
/***/ ((__unused_webpack_module, exports) => {

"use strict";
eval("\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.defaultModelFetcher = void 0;\nconst defaultModelFetcher = (path) => {\n    return fetch(path).then((model) => model.arrayBuffer());\n};\nexports.defaultModelFetcher = defaultModelFetcher;\n//# sourceMappingURL=default-model-fetcher.js.map\n\n//# sourceURL=webpack://vad/./dist/default-model-fetcher.js?");

/***/ }),

/***/ "./dist/frame-processor.js":
/*!*********************************!*\
  !*** ./dist/frame-processor.js ***!
  \*********************************/
/***/ ((__unused_webpack_module, exports, __webpack_require__) => {

"use strict";
eval("\n/*\nSome of this code, together with the default options found in index.ts,\nwere taken (or took inspiration) from https://github.com/snakers4/silero-vad\n*/\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.FrameProcessor = exports.validateOptions = exports.defaultFrameProcessorOptions = void 0;\nconst logging_1 = __webpack_require__(/*! ./logging */ \"./dist/logging.js\");\nconst messages_1 = __webpack_require__(/*! ./messages */ \"./dist/messages.js\");\nexports.defaultFrameProcessorOptions = {\n    positiveSpeechThreshold: 0.3,\n    negativeSpeechThreshold: 0.25,\n    preSpeechPadMs: 800,\n    redemptionMs: 1400,\n    minSpeechMs: 400,\n    submitUserSpeechOnPause: false,\n};\nfunction validateOptions(options) {\n    if (options.positiveSpeechThreshold < 0 ||\n        options.positiveSpeechThreshold > 1) {\n        logging_1.log.error(\"positiveSpeechThreshold should be a number between 0 and 1\");\n    }\n    if (options.negativeSpeechThreshold < 0 ||\n        options.negativeSpeechThreshold > options.positiveSpeechThreshold) {\n        logging_1.log.error(\"negativeSpeechThreshold should be between 0 and positiveSpeechThreshold\");\n    }\n    if (options.preSpeechPadMs < 0) {\n        logging_1.log.error(\"preSpeechPadMs should be positive\");\n    }\n    if (options.redemptionMs < 0) {\n        logging_1.log.error(\"redemptionMs should be positive\");\n    }\n    if (options.minSpeechMs < 0) {\n        logging_1.log.error(\"minSpeechMs should be positive\");\n    }\n}\nexports.validateOptions = validateOptions;\nconst concatArrays = (arrays) => {\n    const sizes = arrays.reduce((out, next) => {\n        out.push(out.at(-1) + next.length);\n        return out;\n    }, [0]);\n    const outArray = new Float32Array(sizes.at(-1));\n    arrays.forEach((arr, index) => {\n        const place = sizes[index];\n        outArray.set(arr, place);\n    });\n    return outArray;\n};\nfunction calculateFrameParams(options, msPerFrame) {\n    const redemptionFrames = Math.floor(options.redemptionMs / msPerFrame);\n    const preSpeechPadFrames = Math.floor(options.preSpeechPadMs / msPerFrame);\n    const minSpeechFrames = Math.floor(options.minSpeechMs / msPerFrame);\n    return { redemptionFrames, preSpeechPadFrames, minSpeechFrames };\n}\nclass FrameProcessor {\n    constructor(modelProcessFunc, modelResetFunc, options, msPerFrame) {\n        this.modelProcessFunc = modelProcessFunc;\n        this.modelResetFunc = modelResetFunc;\n        this.options = options;\n        this.msPerFrame = msPerFrame;\n        this.speaking = false;\n        this.redemptionCounter = 0;\n        this.speechFrameCount = 0;\n        this.active = false;\n        this.speechRealStartFired = false;\n        this.setOptions = (update) => {\n            this.options = { ...this.options, ...update };\n            const { redemptionFrames, preSpeechPadFrames, minSpeechFrames } = calculateFrameParams(this.options, this.msPerFrame);\n            this.redemptionFrames = redemptionFrames;\n            this.preSpeechPadFrames = preSpeechPadFrames;\n            this.minSpeechFrames = minSpeechFrames;\n        };\n        this.reset = () => {\n            this.speaking = false;\n            this.speechRealStartFired = false;\n            this.audioBuffer = [];\n            this.modelResetFunc();\n            this.redemptionCounter = 0;\n            this.speechFrameCount = 0;\n        };\n        this.pause = (handleEvent) => {\n            this.active = false;\n            if (this.options.submitUserSpeechOnPause) {\n                this.endSegment(handleEvent);\n            }\n            else {\n                this.reset();\n            }\n        };\n        this.resume = () => {\n            this.active = true;\n        };\n        this.endSegment = (handleEvent) => {\n            const audioBuffer = this.audioBuffer;\n            this.audioBuffer = [];\n            const speaking = this.speaking;\n            this.reset();\n            if (speaking) {\n                const speechFrameCount = audioBuffer.reduce((acc, item) => {\n                    return item.isSpeech ? acc + 1 : acc;\n                }, 0);\n                if (speechFrameCount >= this.minSpeechFrames) {\n                    const audio = concatArrays(audioBuffer.map((item) => item.frame));\n                    handleEvent({ msg: messages_1.Message.SpeechEnd, audio });\n                }\n                else {\n                    handleEvent({ msg: messages_1.Message.VADMisfire });\n                }\n            }\n            return {};\n        };\n        this.process = async (frame, handleEvent) => {\n            if (!this.active) {\n                return;\n            }\n            const probs = await this.modelProcessFunc(frame);\n            const isSpeech = probs.isSpeech >= this.options.positiveSpeechThreshold;\n            handleEvent({ probs, msg: messages_1.Message.FrameProcessed, frame });\n            this.audioBuffer.push({\n                frame,\n                isSpeech,\n            });\n            if (isSpeech) {\n                this.speechFrameCount++;\n                this.redemptionCounter = 0;\n            }\n            if (isSpeech && !this.speaking) {\n                this.speaking = true;\n                handleEvent({ msg: messages_1.Message.SpeechStart });\n            }\n            if (this.speaking &&\n                this.speechFrameCount === this.minSpeechFrames &&\n                !this.speechRealStartFired) {\n                this.speechRealStartFired = true;\n                handleEvent({ msg: messages_1.Message.SpeechRealStart });\n            }\n            if (probs.isSpeech < this.options.negativeSpeechThreshold &&\n                this.speaking &&\n                ++this.redemptionCounter >= this.redemptionFrames) {\n                this.redemptionCounter = 0;\n                this.speechFrameCount = 0;\n                this.speaking = false;\n                this.speechRealStartFired = false;\n                const audioBuffer = this.audioBuffer;\n                this.audioBuffer = [];\n                const speechFrameCount = audioBuffer.reduce((acc, item) => {\n                    return item.isSpeech ? acc + 1 : acc;\n                }, 0);\n                if (speechFrameCount >= this.minSpeechFrames) {\n                    const audio = concatArrays(audioBuffer.map((item) => item.frame));\n                    handleEvent({ msg: messages_1.Message.SpeechEnd, audio });\n                }\n                else {\n                    handleEvent({ msg: messages_1.Message.VADMisfire });\n                }\n            }\n            if (!this.speaking) {\n                while (this.audioBuffer.length > this.preSpeechPadFrames) {\n                    this.audioBuffer.shift();\n                }\n                this.speechFrameCount = 0;\n            }\n        };\n        this.audioBuffer = [];\n        const { redemptionFrames, preSpeechPadFrames, minSpeechFrames } = calculateFrameParams(this.options, this.msPerFrame);\n        this.redemptionFrames = redemptionFrames;\n        this.preSpeechPadFrames = preSpeechPadFrames;\n        this.minSpeechFrames = minSpeechFrames;\n        this.reset();\n    }\n}\nexports.FrameProcessor = FrameProcessor;\n//# sourceMappingURL=frame-processor.js.map\n\n//# sourceURL=webpack://vad/./dist/frame-processor.js?");

/***/ }),

/***/ "./dist/index.js":
/*!***********************!*\
  !*** ./dist/index.js ***!
  \***********************/
/***/ ((__unused_webpack_module, exports, __webpack_require__) => {

"use strict";
eval("\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.getDefaultRealTimeVADOptions = exports.MicVAD = exports.DEFAULT_MODEL = exports.utils = exports.NonRealTimeVAD = exports.Message = exports.FrameProcessor = exports.defaultModelFetcher = exports.baseAssetPath = void 0;\nvar asset_path_1 = __webpack_require__(/*! ./asset-path */ \"./dist/asset-path.js\");\nObject.defineProperty(exports, \"baseAssetPath\", ({ enumerable: true, get: function () { return asset_path_1.baseAssetPath; } }));\nvar default_model_fetcher_1 = __webpack_require__(/*! ./default-model-fetcher */ \"./dist/default-model-fetcher.js\");\nObject.defineProperty(exports, \"defaultModelFetcher\", ({ enumerable: true, get: function () { return default_model_fetcher_1.defaultModelFetcher; } }));\nvar frame_processor_1 = __webpack_require__(/*! ./frame-processor */ \"./dist/frame-processor.js\");\nObject.defineProperty(exports, \"FrameProcessor\", ({ enumerable: true, get: function () { return frame_processor_1.FrameProcessor; } }));\nvar messages_1 = __webpack_require__(/*! ./messages */ \"./dist/messages.js\");\nObject.defineProperty(exports, \"Message\", ({ enumerable: true, get: function () { return messages_1.Message; } }));\nvar non_real_time_vad_1 = __webpack_require__(/*! ./non-real-time-vad */ \"./dist/non-real-time-vad.js\");\nObject.defineProperty(exports, \"NonRealTimeVAD\", ({ enumerable: true, get: function () { return non_real_time_vad_1.NonRealTimeVAD; } }));\nconst utils_1 = __webpack_require__(/*! ./utils */ \"./dist/utils.js\");\nexports.utils = {\n    audioFileToArray: utils_1.audioFileToArray,\n    minFramesForTargetMS: utils_1.minFramesForTargetMS,\n    arrayBufferToBase64: utils_1.arrayBufferToBase64,\n    encodeWAV: utils_1.encodeWAV,\n};\nvar real_time_vad_1 = __webpack_require__(/*! ./real-time-vad */ \"./dist/real-time-vad.js\");\nObject.defineProperty(exports, \"DEFAULT_MODEL\", ({ enumerable: true, get: function () { return real_time_vad_1.DEFAULT_MODEL; } }));\nObject.defineProperty(exports, \"MicVAD\", ({ enumerable: true, get: function () { return real_time_vad_1.MicVAD; } }));\nObject.defineProperty(exports, \"getDefaultRealTimeVADOptions\", ({ enumerable: true, get: function () { return real_time_vad_1.getDefaultRealTimeVADOptions; } }));\n//# sourceMappingURL=index.js.map\n\n//# sourceURL=webpack://vad/./dist/index.js?");

/***/ }),

/***/ "./dist/logging.js":
/*!*************************!*\
  !*** ./dist/logging.js ***!
  \*************************/
/***/ ((__unused_webpack_module, exports) => {

"use strict";
eval("\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.log = void 0;\nconst mkLogger = (level) => (message) => {\n    console.log(`VAD | ${level} >`, message);\n};\nexports.log = {\n    error: mkLogger(\"error\"),\n    debug: mkLogger(\"debug\"),\n    warn: mkLogger(\"warn\"),\n};\n//# sourceMappingURL=logging.js.map\n\n//# sourceURL=webpack://vad/./dist/logging.js?");

/***/ }),

/***/ "./dist/messages.js":
/*!**************************!*\
  !*** ./dist/messages.js ***!
  \**************************/
/***/ ((__unused_webpack_module, exports) => {

"use strict";
eval("\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.Message = void 0;\nvar Message;\n(function (Message) {\n    Message[\"AudioFrame\"] = \"AUDIO_FRAME\";\n    Message[\"SpeechStart\"] = \"SPEECH_START\";\n    Message[\"VADMisfire\"] = \"VAD_MISFIRE\";\n    Message[\"SpeechEnd\"] = \"SPEECH_END\";\n    Message[\"SpeechStop\"] = \"SPEECH_STOP\";\n    Message[\"SpeechRealStart\"] = \"SPEECH_REAL_START\";\n    Message[\"FrameProcessed\"] = \"FRAME_PROCESSED\";\n})(Message || (exports.Message = Message = {}));\n//# sourceMappingURL=messages.js.map\n\n//# sourceURL=webpack://vad/./dist/messages.js?");

/***/ }),

/***/ "./dist/models/common.js":
/*!*******************************!*\
  !*** ./dist/models/common.js ***!
  \*******************************/
/***/ ((__unused_webpack_module, exports) => {

"use strict";
eval("\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\n//# sourceMappingURL=common.js.map\n\n//# sourceURL=webpack://vad/./dist/models/common.js?");

/***/ }),

/***/ "./dist/models/index.js":
/*!******************************!*\
  !*** ./dist/models/index.js ***!
  \******************************/
/***/ (function(__unused_webpack_module, exports, __webpack_require__) {

"use strict";
eval("\nvar __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {\n    if (k2 === undefined) k2 = k;\n    var desc = Object.getOwnPropertyDescriptor(m, k);\n    if (!desc || (\"get\" in desc ? !m.__esModule : desc.writable || desc.configurable)) {\n      desc = { enumerable: true, get: function() { return m[k]; } };\n    }\n    Object.defineProperty(o, k2, desc);\n}) : (function(o, m, k, k2) {\n    if (k2 === undefined) k2 = k;\n    o[k2] = m[k];\n}));\nvar __exportStar = (this && this.__exportStar) || function(m, exports) {\n    for (var p in m) if (p !== \"default\" && !Object.prototype.hasOwnProperty.call(exports, p)) __createBinding(exports, m, p);\n};\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.SileroV5 = exports.SileroLegacy = void 0;\n__exportStar(__webpack_require__(/*! ./common */ \"./dist/models/common.js\"), exports);\nvar legacy_1 = __webpack_require__(/*! ./legacy */ \"./dist/models/legacy.js\");\nObject.defineProperty(exports, \"SileroLegacy\", ({ enumerable: true, get: function () { return legacy_1.SileroLegacy; } }));\nvar v5_1 = __webpack_require__(/*! ./v5 */ \"./dist/models/v5.js\");\nObject.defineProperty(exports, \"SileroV5\", ({ enumerable: true, get: function () { return v5_1.SileroV5; } }));\n//# sourceMappingURL=index.js.map\n\n//# sourceURL=webpack://vad/./dist/models/index.js?");

/***/ }),

/***/ "./dist/models/legacy.js":
/*!*******************************!*\
  !*** ./dist/models/legacy.js ***!
  \*******************************/
/***/ ((__unused_webpack_module, exports, __webpack_require__) => {

"use strict";
eval("\nvar _a;\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.SileroLegacy = void 0;\nconst logging_1 = __webpack_require__(/*! ../logging */ \"./dist/logging.js\");\nclass SileroLegacy {\n    constructor(ortInstance, _session, _h, _c, _sr) {\n        this.ortInstance = ortInstance;\n        this._session = _session;\n        this._h = _h;\n        this._c = _c;\n        this._sr = _sr;\n        this.reset_state = () => {\n            const zeroes = Array(2 * 64).fill(0);\n            this._h = new this.ortInstance.Tensor(\"float32\", zeroes, [2, 1, 64]);\n            this._c = new this.ortInstance.Tensor(\"float32\", zeroes, [2, 1, 64]);\n        };\n        this.process = async (audioFrame) => {\n            const t = new this.ortInstance.Tensor(\"float32\", audioFrame, [\n                1,\n                audioFrame.length,\n            ]);\n            const inputs = {\n                input: t,\n                h: this._h,\n                c: this._c,\n                sr: this._sr,\n            };\n            const out = await this._session.run(inputs);\n            this._h = out[\"hn\"];\n            this._c = out[\"cn\"];\n            const [isSpeech] = out[\"output\"]?.data;\n            const notSpeech = 1 - isSpeech;\n            return { notSpeech, isSpeech };\n        };\n        this.release = async () => {\n            await this._session.release();\n            this._h.dispose();\n            this._c.dispose();\n            this._sr.dispose();\n        };\n    }\n}\nexports.SileroLegacy = SileroLegacy;\n_a = SileroLegacy;\nSileroLegacy.new = async (ortInstance, modelFetcher) => {\n    logging_1.log.debug(\"initializing vad\");\n    const modelArrayBuffer = await modelFetcher();\n    const _session = await ortInstance.InferenceSession.create(modelArrayBuffer);\n    const _sr = new ortInstance.Tensor(\"int64\", [16000n]);\n    const zeroes = Array(2 * 64).fill(0);\n    const _h = new ortInstance.Tensor(\"float32\", zeroes, [2, 1, 64]);\n    const _c = new ortInstance.Tensor(\"float32\", zeroes, [2, 1, 64]);\n    logging_1.log.debug(\"vad is initialized\");\n    const model = new _a(ortInstance, _session, _h, _c, _sr);\n    return model;\n};\n//# sourceMappingURL=legacy.js.map\n\n//# sourceURL=webpack://vad/./dist/models/legacy.js?");

/***/ }),

/***/ "./dist/models/v5.js":
/*!***************************!*\
  !*** ./dist/models/v5.js ***!
  \***************************/
/***/ ((__unused_webpack_module, exports, __webpack_require__) => {

"use strict";
eval("\nvar _a;\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.SileroV5 = void 0;\nconst logging_1 = __webpack_require__(/*! ../logging */ \"./dist/logging.js\");\nfunction getNewState(ortInstance) {\n    const zeroes = Array(2 * 128).fill(0);\n    return new ortInstance.Tensor(\"float32\", zeroes, [2, 1, 128]);\n}\nclass SileroV5 {\n    constructor(_session, _state, _sr, ortInstance) {\n        this._session = _session;\n        this._state = _state;\n        this._sr = _sr;\n        this.ortInstance = ortInstance;\n        this.reset_state = () => {\n            this._state = getNewState(this.ortInstance);\n        };\n        this.process = async (audioFrame) => {\n            const t = new this.ortInstance.Tensor(\"float32\", audioFrame, [\n                1,\n                audioFrame.length,\n            ]);\n            const inputs = {\n                input: t,\n                state: this._state,\n                sr: this._sr,\n            };\n            const out = await this._session.run(inputs);\n            if (!out[\"stateN\"]) {\n                throw new Error(\"No state from model\");\n            }\n            this._state = out[\"stateN\"];\n            if (!out[\"output\"]?.data) {\n                throw new Error(\"No output from model\");\n            }\n            const isSpeech = out[\"output\"].data[0];\n            if (typeof isSpeech != \"number\") {\n                throw new Error(\"Weird output data\");\n            }\n            const notSpeech = 1 - isSpeech;\n            return { notSpeech, isSpeech };\n        };\n        this.release = async () => {\n            await this._session.release();\n            this._state.dispose();\n            this._sr.dispose();\n        };\n    }\n}\nexports.SileroV5 = SileroV5;\n_a = SileroV5;\nSileroV5.new = async (ortInstance, modelFetcher) => {\n    logging_1.log.debug(\"Loading VAD...\");\n    const modelArrayBuffer = await modelFetcher();\n    const _session = await ortInstance.InferenceSession.create(modelArrayBuffer);\n    const _sr = new ortInstance.Tensor(\"int64\", [16000n]);\n    const _state = getNewState(ortInstance);\n    logging_1.log.debug(\"...finished loading VAD\");\n    return new _a(_session, _state, _sr, ortInstance);\n};\n//# sourceMappingURL=v5.js.map\n\n//# sourceURL=webpack://vad/./dist/models/v5.js?");

/***/ }),

/***/ "./dist/non-real-time-vad.js":
/*!***********************************!*\
  !*** ./dist/non-real-time-vad.js ***!
  \***********************************/
/***/ (function(__unused_webpack_module, exports, __webpack_require__) {

"use strict";
eval("\nvar __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {\n    if (k2 === undefined) k2 = k;\n    var desc = Object.getOwnPropertyDescriptor(m, k);\n    if (!desc || (\"get\" in desc ? !m.__esModule : desc.writable || desc.configurable)) {\n      desc = { enumerable: true, get: function() { return m[k]; } };\n    }\n    Object.defineProperty(o, k2, desc);\n}) : (function(o, m, k, k2) {\n    if (k2 === undefined) k2 = k;\n    o[k2] = m[k];\n}));\nvar __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {\n    Object.defineProperty(o, \"default\", { enumerable: true, value: v });\n}) : function(o, v) {\n    o[\"default\"] = v;\n});\nvar __importStar = (this && this.__importStar) || function (mod) {\n    if (mod && mod.__esModule) return mod;\n    var result = {};\n    if (mod != null) for (var k in mod) if (k !== \"default\" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);\n    __setModuleDefault(result, mod);\n    return result;\n};\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.NonRealTimeVAD = exports.defaultNonRealTimeVADOptions = void 0;\nconst ortInstance = __importStar(__webpack_require__(/*! onnxruntime-web */ \"onnxruntime-web\"));\nconst asset_path_1 = __webpack_require__(/*! ./asset-path */ \"./dist/asset-path.js\");\nconst default_model_fetcher_1 = __webpack_require__(/*! ./default-model-fetcher */ \"./dist/default-model-fetcher.js\");\nconst frame_processor_1 = __webpack_require__(/*! ./frame-processor */ \"./dist/frame-processor.js\");\nconst messages_1 = __webpack_require__(/*! ./messages */ \"./dist/messages.js\");\nconst models_1 = __webpack_require__(/*! ./models */ \"./dist/models/index.js\");\nconst resampler_1 = __webpack_require__(/*! ./resampler */ \"./dist/resampler.js\");\nexports.defaultNonRealTimeVADOptions = {\n    ...frame_processor_1.defaultFrameProcessorOptions,\n    modelURL: asset_path_1.baseAssetPath + \"silero_vad_legacy.onnx\",\n    modelFetcher: default_model_fetcher_1.defaultModelFetcher,\n};\nclass NonRealTimeVAD {\n    static async new(options = {}) {\n        const fullOptions = {\n            ...exports.defaultNonRealTimeVADOptions,\n            ...options,\n        };\n        (0, frame_processor_1.validateOptions)(fullOptions);\n        if (fullOptions.ortConfig !== undefined) {\n            fullOptions.ortConfig(ortInstance);\n        }\n        const modelFetcher = () => fullOptions.modelFetcher(fullOptions.modelURL);\n        const model = await models_1.SileroLegacy.new(ortInstance, modelFetcher);\n        const frameProcessor = new frame_processor_1.FrameProcessor(model.process, model.reset_state, {\n            positiveSpeechThreshold: fullOptions.positiveSpeechThreshold,\n            negativeSpeechThreshold: fullOptions.negativeSpeechThreshold,\n            redemptionMs: fullOptions.redemptionMs,\n            preSpeechPadMs: fullOptions.preSpeechPadMs,\n            minSpeechMs: fullOptions.minSpeechMs,\n            submitUserSpeechOnPause: fullOptions.submitUserSpeechOnPause,\n        }, 1536 / 16);\n        frameProcessor.resume();\n        const vad = new this(modelFetcher, ortInstance, fullOptions, frameProcessor);\n        return vad;\n    }\n    constructor(modelFetcher, ort, options, frameProcessor) {\n        this.modelFetcher = modelFetcher;\n        this.ort = ort;\n        this.options = options;\n        this.frameProcessor = frameProcessor;\n        this.frameSamples = 1536;\n    }\n    async *run(inputAudio, sampleRate) {\n        const resamplerOptions = {\n            nativeSampleRate: sampleRate,\n            targetSampleRate: 16000,\n            targetFrameSize: this.frameSamples,\n        };\n        const resampler = new resampler_1.Resampler(resamplerOptions);\n        let start = 0;\n        let end = 0;\n        let frameIndex = 0;\n        for await (const frame of resampler.stream(inputAudio)) {\n            const messageContainer = [];\n            await this.frameProcessor.process(frame, (event) => {\n                messageContainer.push(event);\n            });\n            for (const event of messageContainer) {\n                switch (event.msg) {\n                    case messages_1.Message.SpeechStart:\n                        start = (frameIndex * this.frameSamples) / 16;\n                        break;\n                    case messages_1.Message.SpeechEnd:\n                        end = ((frameIndex + 1) * this.frameSamples) / 16;\n                        yield { audio: event.audio, start, end };\n                        break;\n                    default:\n                        break;\n                }\n            }\n            frameIndex++;\n        }\n        const messageContainer = [];\n        this.frameProcessor.endSegment((event) => {\n            messageContainer.push(event);\n        });\n        for (const event of messageContainer) {\n            switch (event.msg) {\n                case messages_1.Message.SpeechEnd:\n                    yield {\n                        audio: event.audio,\n                        start,\n                        end: (frameIndex * this.frameSamples) / 16,\n                    };\n            }\n        }\n    }\n}\nexports.NonRealTimeVAD = NonRealTimeVAD;\n//# sourceMappingURL=non-real-time-vad.js.map\n\n//# sourceURL=webpack://vad/./dist/non-real-time-vad.js?");

/***/ }),

/***/ "./dist/real-time-vad.js":
/*!*******************************!*\
  !*** ./dist/real-time-vad.js ***!
  \*******************************/
/***/ (function(__unused_webpack_module, exports, __webpack_require__) {

"use strict";
eval("\nvar __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {\n    if (k2 === undefined) k2 = k;\n    var desc = Object.getOwnPropertyDescriptor(m, k);\n    if (!desc || (\"get\" in desc ? !m.__esModule : desc.writable || desc.configurable)) {\n      desc = { enumerable: true, get: function() { return m[k]; } };\n    }\n    Object.defineProperty(o, k2, desc);\n}) : (function(o, m, k, k2) {\n    if (k2 === undefined) k2 = k;\n    o[k2] = m[k];\n}));\nvar __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {\n    Object.defineProperty(o, \"default\", { enumerable: true, value: v });\n}) : function(o, v) {\n    o[\"default\"] = v;\n});\nvar __importStar = (this && this.__importStar) || function (mod) {\n    if (mod && mod.__esModule) return mod;\n    var result = {};\n    if (mod != null) for (var k in mod) if (k !== \"default\" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);\n    __setModuleDefault(result, mod);\n    return result;\n};\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.MicVAD = exports.getDefaultRealTimeVADOptions = exports.ort = exports.DEFAULT_MODEL = void 0;\nconst ortInstance = __importStar(__webpack_require__(/*! onnxruntime-web/wasm */ \"../../node_modules/onnxruntime-web/dist/ort.wasm.min.js\"));\nconst default_model_fetcher_1 = __webpack_require__(/*! ./default-model-fetcher */ \"./dist/default-model-fetcher.js\");\nconst frame_processor_1 = __webpack_require__(/*! ./frame-processor */ \"./dist/frame-processor.js\");\nconst logging_1 = __webpack_require__(/*! ./logging */ \"./dist/logging.js\");\nconst messages_1 = __webpack_require__(/*! ./messages */ \"./dist/messages.js\");\nconst models_1 = __webpack_require__(/*! ./models */ \"./dist/models/index.js\");\nconst resampler_1 = __webpack_require__(/*! ./resampler */ \"./dist/resampler.js\");\nexports.DEFAULT_MODEL = \"legacy\";\nexports.ort = ortInstance;\nconst workletFile = \"vad.worklet.bundle.min.js\";\nconst sileroV5File = \"silero_vad_v5.onnx\";\nconst sileroLegacyFile = \"silero_vad_legacy.onnx\";\nconst getDefaultRealTimeVADOptions = (model) => {\n    return {\n        ...frame_processor_1.defaultFrameProcessorOptions,\n        onFrameProcessed: () => { },\n        onVADMisfire: () => {\n            logging_1.log.debug(\"VAD misfire\");\n        },\n        onSpeechStart: () => {\n            logging_1.log.debug(\"Detected speech start\");\n        },\n        onSpeechEnd: () => {\n            logging_1.log.debug(\"Detected speech end\");\n        },\n        onSpeechRealStart: () => {\n            logging_1.log.debug(\"Detected real speech start\");\n        },\n        baseAssetPath: \"./\",\n        onnxWASMBasePath: \"./\",\n        model: model,\n        workletOptions: {},\n        getStream: async () => {\n            const stream = await navigator.mediaDevices.getUserMedia({\n                audio: {\n                    channelCount: 1,\n                    echoCancellation: true,\n                    autoGainControl: true,\n                    noiseSuppression: true,\n                },\n            });\n            return stream;\n        },\n        pauseStream: async (_stream) => {\n            _stream.getTracks().forEach((track) => {\n                track.stop();\n            });\n        },\n        resumeStream: async () => {\n            const stream = await navigator.mediaDevices.getUserMedia({\n                audio: {\n                    channelCount: 1,\n                    echoCancellation: true,\n                    autoGainControl: true,\n                    noiseSuppression: true,\n                },\n            });\n            return stream;\n        },\n        ortConfig: (ort) => {\n            ort.env.logLevel = \"error\";\n        },\n        startOnLoad: true,\n        processorType: \"auto\",\n    };\n};\nexports.getDefaultRealTimeVADOptions = getDefaultRealTimeVADOptions;\nconst detectProcessorType = (ctx) => {\n    if (\"audioWorklet\" in ctx && typeof AudioWorkletNode === \"function\") {\n        return \"AudioWorklet\";\n    }\n    return \"ScriptProcessor\";\n};\nasync function getVADNodeAsWorklet(workletURL, workletOptions, audioContext, frameSamples, processFrame) {\n    await audioContext.audioWorklet.addModule(workletURL);\n    // eslint-disable-next-line @typescript-eslint/no-unsafe-assignment\n    workletOptions.processorOptions = {\n        ...(workletOptions.processorOptions ?? {}),\n        frameSamples: frameSamples,\n    };\n    const audioNode = new AudioWorkletNode(audioContext, \"vad-helper-worklet\", workletOptions);\n    audioNode.port.onmessage = async (ev) => {\n        const data = ev.data;\n        if (!(typeof data === \"object\" && data && \"message\" in data)) {\n            console.error(\"Invalid message event\", data);\n            return;\n        }\n        switch (data.message) {\n            case messages_1.Message.AudioFrame: {\n                if (!(\"data\" in data && data.data instanceof ArrayBuffer)) {\n                    console.log(\"Audio frame message has no data\");\n                    return;\n                }\n                const frame = new Float32Array(data.data);\n                await processFrame(frame);\n                break;\n            }\n        }\n    };\n    return audioNode;\n}\nasync function getVADNodeAsScriptProcessor(audioContext, frameSamples, processFrame) {\n    const resampler = new resampler_1.Resampler({\n        nativeSampleRate: audioContext.sampleRate,\n        targetSampleRate: 16000,\n        targetFrameSize: frameSamples,\n    });\n    logging_1.log.debug(\"using script processor\");\n    // Fallback to ScriptProcessor\n    const bufferSize = 4096; // Increased for more stable processing\n    const audioNode = audioContext.createScriptProcessor(bufferSize, 1, 1);\n    let processingAudio = false;\n    audioNode.onaudioprocess = async (e) => {\n        if (processingAudio)\n            return;\n        processingAudio = true;\n        try {\n            const input = e.inputBuffer.getChannelData(0);\n            const output = e.outputBuffer.getChannelData(0);\n            output.fill(0);\n            // Process through resampler\n            const frames = resampler.process(input);\n            for (const frame of frames) {\n                await processFrame(frame);\n            }\n        }\n        catch (error) {\n            console.error(\"Error processing audio:\", error);\n        }\n        finally {\n            processingAudio = false;\n        }\n    };\n    // https://github.com/WebAudio/web-audio-api/issues/345\n    // -> we need to connect an output or will not work due to chrome bug\n    audioNode.connect(audioContext.destination);\n    return audioNode;\n}\nclass MicVAD {\n    constructor(options, frameProcessor, model, frameSamples, listening = false, errored = null, _stream = null, _audioContext = null, _vadNode = null, _mediaStreamAudioSourceNode = null, _audioProcessorAdapterType = null, initializationState = \"uninitialized\", ownsAudioContext = false) {\n        this.options = options;\n        this.frameProcessor = frameProcessor;\n        this.model = model;\n        this.frameSamples = frameSamples;\n        this.listening = listening;\n        this.errored = errored;\n        this._stream = _stream;\n        this._audioContext = _audioContext;\n        this._vadNode = _vadNode;\n        this._mediaStreamAudioSourceNode = _mediaStreamAudioSourceNode;\n        this._audioProcessorAdapterType = _audioProcessorAdapterType;\n        this.initializationState = initializationState;\n        this.ownsAudioContext = ownsAudioContext;\n        this.getAudioInstances = () => {\n            if (this._stream === null ||\n                this._audioContext === null ||\n                this._vadNode == null ||\n                this._mediaStreamAudioSourceNode == null) {\n                throw new Error(\"MicVAD has null stream, audio context, or processor adapter\");\n            }\n            return {\n                stream: this._stream,\n                audioContext: this._audioContext,\n                vadNode: this._vadNode,\n                mediaStreamAudioSourceNode: this._mediaStreamAudioSourceNode,\n            };\n        };\n        this.setErrored = (error) => {\n            this.initializationState = \"errored\";\n            this.errored = error;\n        };\n        this.start = async () => {\n            switch (this.initializationState) {\n                case \"uninitialized\": {\n                    logging_1.log.debug(\"initializing micVAD\");\n                    this.initializationState = \"initializing\";\n                    this.frameProcessor.resume();\n                    try {\n                        this._stream = await this.options.getStream();\n                    }\n                    catch (error) {\n                        if (error instanceof Error) {\n                            this.setErrored(error.message);\n                        }\n                        else {\n                            this.setErrored(String(error));\n                        }\n                        throw error;\n                    }\n                    if (this.options.audioContext) {\n                        console.log(\"using custom audio context\");\n                        this._audioContext = this.options.audioContext;\n                    }\n                    else {\n                        console.log(\"using default audio context\");\n                        this._audioContext = new AudioContext();\n                        this.ownsAudioContext = true;\n                    }\n                    // eslint-disable-next-line @typescript-eslint/no-unnecessary-condition\n                    if (!this._audioContext) {\n                        this.setErrored(\"Audio context is null\");\n                        throw Error(\"Audio context is null\");\n                    }\n                    this._audioProcessorAdapterType =\n                        this.options.processorType == \"auto\"\n                            ? detectProcessorType(this._audioContext)\n                            : this.options.processorType;\n                    switch (this._audioProcessorAdapterType) {\n                        case \"AudioWorklet\":\n                            {\n                                this._vadNode = await getVADNodeAsWorklet(this.options.baseAssetPath + workletFile, this.options.workletOptions, this._audioContext, this.frameSamples, this.processFrame);\n                            }\n                            break;\n                        case \"ScriptProcessor\":\n                            {\n                                this._vadNode = await getVADNodeAsScriptProcessor(this._audioContext, this.frameSamples, this.processFrame);\n                            }\n                            break;\n                        default: {\n                            throw new Error(\n                            // eslint-disable-next-line @typescript-eslint/restrict-template-expressions\n                            `Unsupported audio processor adapter type: ${this._audioProcessorAdapterType}`);\n                        }\n                    }\n                    this._mediaStreamAudioSourceNode = new MediaStreamAudioSourceNode(this._audioContext, {\n                        mediaStream: this._stream,\n                    });\n                    this._mediaStreamAudioSourceNode.connect(this._vadNode);\n                    logging_1.log.debug(\"started micVAD\");\n                    this.listening = true;\n                    this.initializationState = \"initialized\";\n                    break;\n                }\n                case \"initializing\": {\n                    logging_1.log.warn(\"start called while initializing\");\n                    break;\n                }\n                case \"initialized\": {\n                    if (this.listening) {\n                        return;\n                    }\n                    this.listening = true;\n                    this.frameProcessor.resume();\n                    const { stream, audioContext, vadNode } = this.getAudioInstances();\n                    this._stream = await this.options.resumeStream(stream);\n                    const mediaStreamAudioSourceNode = new MediaStreamAudioSourceNode(audioContext, { mediaStream: this._stream });\n                    this._mediaStreamAudioSourceNode = mediaStreamAudioSourceNode;\n                    mediaStreamAudioSourceNode.connect(vadNode);\n                    break;\n                }\n                case \"destroyed\": {\n                    logging_1.log.warn(\"start called after destroyed\");\n                    break;\n                }\n                case \"errored\": {\n                    logging_1.log.error(\"start called after errored\");\n                    break;\n                }\n                default: {\n                    logging_1.log.warn(\"weird initialization state\");\n                    break;\n                }\n            }\n        };\n        this.pause = async () => {\n            if (!this.listening) {\n                return;\n            }\n            this.listening = false;\n            const { stream, mediaStreamAudioSourceNode } = this.getAudioInstances();\n            await this.options.pauseStream(stream);\n            mediaStreamAudioSourceNode.disconnect();\n            this.frameProcessor.pause(this.handleFrameProcessorEvent);\n        };\n        this.destroy = async () => {\n            logging_1.log.debug(\"destroy called\");\n            this.initializationState = \"destroyed\";\n            const { vadNode } = this.getAudioInstances();\n            if (vadNode instanceof AudioWorkletNode) {\n                vadNode.port.postMessage(messages_1.Message.SpeechStop);\n            }\n            if (this.listening) {\n                await this.pause();\n            }\n            await this.model.release();\n            if (this.ownsAudioContext) {\n                await this._audioContext?.close();\n            }\n        };\n        this.setOptions = (update) => {\n            this.frameProcessor.setOptions(update);\n        };\n        this.processFrame = async (frame) => {\n            await this.frameProcessor.process(frame, this.handleFrameProcessorEvent);\n        };\n        this.handleFrameProcessorEvent = (ev) => {\n            switch (ev.msg) {\n                case messages_1.Message.FrameProcessed:\n                    void this.options.onFrameProcessed(ev.probs, ev.frame);\n                    break;\n                case messages_1.Message.SpeechStart:\n                    void this.options.onSpeechStart();\n                    break;\n                case messages_1.Message.SpeechRealStart:\n                    void this.options.onSpeechRealStart();\n                    break;\n                case messages_1.Message.VADMisfire:\n                    void this.options.onVADMisfire();\n                    break;\n                case messages_1.Message.SpeechEnd:\n                    void this.options.onSpeechEnd(ev.audio);\n                    break;\n            }\n        };\n    }\n    static async new(options = {}) {\n        const fullOptions = {\n            ...(0, exports.getDefaultRealTimeVADOptions)(options.model ?? exports.DEFAULT_MODEL),\n            ...options,\n        };\n        (0, frame_processor_1.validateOptions)(fullOptions);\n        exports.ort.env.wasm.wasmPaths = fullOptions.onnxWASMBasePath;\n        if (fullOptions.ortConfig !== undefined) {\n            fullOptions.ortConfig(exports.ort);\n        }\n        const modelFile = fullOptions.model === \"v5\" ? sileroV5File : sileroLegacyFile;\n        const modelURL = fullOptions.baseAssetPath + modelFile;\n        const modelFactory = fullOptions.model === \"v5\" ? models_1.SileroV5.new : models_1.SileroLegacy.new;\n        let model;\n        try {\n            model = await modelFactory(exports.ort, () => (0, default_model_fetcher_1.defaultModelFetcher)(modelURL));\n        }\n        catch (e) {\n            console.error(`Encountered an error while loading model file ${modelURL}`);\n            throw e;\n        }\n        const frameSamples = fullOptions.model === \"v5\" ? 512 : 1536;\n        const msPerFrame = frameSamples / 16;\n        const frameProcessor = new frame_processor_1.FrameProcessor(model.process, model.reset_state, {\n            positiveSpeechThreshold: fullOptions.positiveSpeechThreshold,\n            negativeSpeechThreshold: fullOptions.negativeSpeechThreshold,\n            redemptionMs: fullOptions.redemptionMs,\n            preSpeechPadMs: fullOptions.preSpeechPadMs,\n            minSpeechMs: fullOptions.minSpeechMs,\n            submitUserSpeechOnPause: fullOptions.submitUserSpeechOnPause,\n        }, msPerFrame);\n        const micVad = new MicVAD(fullOptions, frameProcessor, model, frameSamples);\n        // things would be simpler if we didn't have to startOnLoad by default, but we are locked in\n        if (fullOptions.startOnLoad) {\n            try {\n                await micVad.start();\n            }\n            catch (e) {\n                console.error(\"Error starting micVad\", e);\n                throw e;\n            }\n        }\n        return micVad;\n    }\n}\nexports.MicVAD = MicVAD;\n//# sourceMappingURL=real-time-vad.js.map\n\n//# sourceURL=webpack://vad/./dist/real-time-vad.js?");

/***/ }),

/***/ "./dist/resampler.js":
/*!***************************!*\
  !*** ./dist/resampler.js ***!
  \***************************/
/***/ ((__unused_webpack_module, exports, __webpack_require__) => {

"use strict";
eval("\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.Resampler = void 0;\nconst logging_1 = __webpack_require__(/*! ./logging */ \"./dist/logging.js\");\nclass Resampler {\n    constructor(options) {\n        this.options = options;\n        this.process = (audioFrame) => {\n            const outputFrames = [];\n            for (const sample of audioFrame) {\n                this.inputBuffer.push(sample);\n                while (this.hasEnoughDataForFrame()) {\n                    const outputFrame = this.generateOutputFrame();\n                    outputFrames.push(outputFrame);\n                }\n            }\n            return outputFrames;\n        };\n        if (options.nativeSampleRate < 16000) {\n            logging_1.log.error(\"nativeSampleRate is too low. Should have 16000 = targetSampleRate <= nativeSampleRate\");\n        }\n        this.inputBuffer = [];\n    }\n    async *stream(audioInput) {\n        for (const sample of audioInput) {\n            this.inputBuffer.push(sample);\n            while (this.hasEnoughDataForFrame()) {\n                const outputFrame = this.generateOutputFrame();\n                yield outputFrame;\n            }\n        }\n    }\n    hasEnoughDataForFrame() {\n        return ((this.inputBuffer.length * this.options.targetSampleRate) /\n            this.options.nativeSampleRate >=\n            this.options.targetFrameSize);\n    }\n    generateOutputFrame() {\n        const outputFrame = new Float32Array(this.options.targetFrameSize);\n        let outputIndex = 0;\n        let inputIndex = 0;\n        while (outputIndex < this.options.targetFrameSize) {\n            let sum = 0;\n            let num = 0;\n            while (inputIndex <\n                Math.min(this.inputBuffer.length, ((outputIndex + 1) * this.options.nativeSampleRate) /\n                    this.options.targetSampleRate)) {\n                const value = this.inputBuffer[inputIndex];\n                if (value !== undefined) {\n                    sum += value;\n                    num++;\n                }\n                inputIndex++;\n            }\n            outputFrame[outputIndex] = sum / num;\n            outputIndex++;\n        }\n        this.inputBuffer = this.inputBuffer.slice(inputIndex);\n        return outputFrame;\n    }\n}\nexports.Resampler = Resampler;\n//# sourceMappingURL=resampler.js.map\n\n//# sourceURL=webpack://vad/./dist/resampler.js?");

/***/ }),

/***/ "./dist/utils.js":
/*!***********************!*\
  !*** ./dist/utils.js ***!
  \***********************/
/***/ ((__unused_webpack_module, exports) => {

"use strict";
eval("\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.audioFileToArray = exports.encodeWAV = exports.arrayBufferToBase64 = exports.minFramesForTargetMS = void 0;\nfunction minFramesForTargetMS(targetDuration, frameSamples, sr = 16000) {\n    return Math.ceil((targetDuration * sr) / 1000 / frameSamples);\n}\nexports.minFramesForTargetMS = minFramesForTargetMS;\nfunction arrayBufferToBase64(buffer) {\n    const bytes = new Uint8Array(buffer);\n    const len = bytes.byteLength;\n    const binary = new Array(len);\n    for (let i = 0; i < len; i++) {\n        const byte = bytes[i];\n        if (byte === undefined) {\n            break;\n        }\n        binary[i] = String.fromCharCode(byte);\n    }\n    return btoa(binary.join(\"\"));\n}\nexports.arrayBufferToBase64 = arrayBufferToBase64;\n/*\nThis rest of this was mostly copied from https://github.com/linto-ai/WebVoiceSDK\n*/\nfunction encodeWAV(samples, format = 3, sampleRate = 16000, numChannels = 1, bitDepth = 32) {\n    const bytesPerSample = bitDepth / 8;\n    const blockAlign = numChannels * bytesPerSample;\n    const buffer = new ArrayBuffer(44 + samples.length * bytesPerSample);\n    const view = new DataView(buffer);\n    /* RIFF identifier */\n    writeString(view, 0, \"RIFF\");\n    /* RIFF chunk length */\n    view.setUint32(4, 36 + samples.length * bytesPerSample, true);\n    /* RIFF type */\n    writeString(view, 8, \"WAVE\");\n    /* format chunk identifier */\n    writeString(view, 12, \"fmt \");\n    /* format chunk length */\n    view.setUint32(16, 16, true);\n    /* sample format (raw) */\n    view.setUint16(20, format, true);\n    /* channel count */\n    view.setUint16(22, numChannels, true);\n    /* sample rate */\n    view.setUint32(24, sampleRate, true);\n    /* byte rate (sample rate * block align) */\n    view.setUint32(28, sampleRate * blockAlign, true);\n    /* block align (channel count * bytes per sample) */\n    view.setUint16(32, blockAlign, true);\n    /* bits per sample */\n    view.setUint16(34, bitDepth, true);\n    /* data chunk identifier */\n    writeString(view, 36, \"data\");\n    /* data chunk length */\n    view.setUint32(40, samples.length * bytesPerSample, true);\n    if (format === 1) {\n        // Raw PCM\n        floatTo16BitPCM(view, 44, samples);\n    }\n    else {\n        writeFloat32(view, 44, samples);\n    }\n    return buffer;\n}\nexports.encodeWAV = encodeWAV;\nfunction writeFloat32(output, offset, input) {\n    for (let i = 0; i < input.length; i++, offset += 4) {\n        output.setFloat32(offset, input[i], true);\n    }\n}\nfunction floatTo16BitPCM(output, offset, input) {\n    for (let i = 0; i < input.length; i++, offset += 2) {\n        const s = Math.max(-1, Math.min(1, input[i]));\n        output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);\n    }\n}\nfunction writeString(view, offset, string) {\n    for (let i = 0; i < string.length; i++) {\n        view.setUint8(offset + i, string.charCodeAt(i));\n    }\n}\nasync function audioFileToArray(audioFileData) {\n    const ctx = new OfflineAudioContext(1, 1, 44100);\n    const reader = new FileReader();\n    let audioBuffer = null;\n    await new Promise((res) => {\n        reader.addEventListener(\"loadend\", () => {\n            const audioData = reader.result;\n            void ctx.decodeAudioData(audioData, (buffer) => {\n                audioBuffer = buffer;\n                ctx\n                    .startRendering()\n                    .then(() => {\n                    console.log(\"Rendering completed successfully\");\n                    res();\n                })\n                    .catch((err) => {\n                    console.error(\"Rendering failed: \", err);\n                });\n            }, (e) => {\n                console.log(\"Error with decoding audio data: \", e);\n            });\n        });\n        reader.readAsArrayBuffer(audioFileData);\n    });\n    // eslint-disable-next-line @typescript-eslint/no-unnecessary-condition\n    if (audioBuffer === null) {\n        throw Error(\"some shit\");\n    }\n    const _audioBuffer = audioBuffer;\n    const out = new Float32Array(_audioBuffer.length);\n    for (let i = 0; i < _audioBuffer.length; i++) {\n        for (let j = 0; j < _audioBuffer.numberOfChannels; j++) {\n            const sample = _audioBuffer.getChannelData(j)[i];\n            const current = out[i];\n            if (sample === undefined || current === undefined) {\n                throw new Error(\"sample or out[i] is undefined\");\n            }\n            out[i] = current + sample;\n        }\n    }\n    return { audio: out, sampleRate: _audioBuffer.sampleRate };\n}\nexports.audioFileToArray = audioFileToArray;\n//# sourceMappingURL=utils.js.map\n\n//# sourceURL=webpack://vad/./dist/utils.js?");

/***/ }),

/***/ "onnxruntime-web":
/*!******************************************************************************************************************!*\
  !*** external {"commonjs":"onnxruntime-web","commonjs2":"onnxruntime-web","amd":"onnxruntime-web","root":"ort"} ***!
  \******************************************************************************************************************/
/***/ ((module) => {

"use strict";
module.exports = __WEBPACK_EXTERNAL_MODULE_onnxruntime_web__;

/***/ })

/******/ 	});
/************************************************************************/
/******/ 	// The module cache
/******/ 	var __webpack_module_cache__ = {};
/******/ 	
/******/ 	// The require function
/******/ 	function __webpack_require__(moduleId) {
/******/ 		// Check if module is in cache
/******/ 		var cachedModule = __webpack_module_cache__[moduleId];
/******/ 		if (cachedModule !== undefined) {
/******/ 			return cachedModule.exports;
/******/ 		}
/******/ 		// Create a new module (and put it into the cache)
/******/ 		var module = __webpack_module_cache__[moduleId] = {
/******/ 			// no module.id needed
/******/ 			// no module.loaded needed
/******/ 			exports: {}
/******/ 		};
/******/ 	
/******/ 		// Execute the module function
/******/ 		__webpack_modules__[moduleId].call(module.exports, module, module.exports, __webpack_require__);
/******/ 	
/******/ 		// Return the exports of the module
/******/ 		return module.exports;
/******/ 	}
/******/ 	
/************************************************************************/
/******/ 	/* webpack/runtime/hasOwnProperty shorthand */
/******/ 	(() => {
/******/ 		__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))
/******/ 	})();
/******/ 	
/************************************************************************/
/******/ 	
/******/ 	// startup
/******/ 	// Load entry module and return exports
/******/ 	// This entry module can't be inlined because the eval devtool is used.
/******/ 	var __webpack_exports__ = __webpack_require__("./dist/index.js");
/******/ 	
/******/ 	return __webpack_exports__;
/******/ })()
;
});